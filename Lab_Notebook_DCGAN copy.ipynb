{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42cf042b",
   "metadata": {},
   "source": [
    "# DCGAN for Microstructure Generation on Micro2D Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9cddc",
   "metadata": {},
   "source": [
    "### Objective\n",
    "To implement and evaluate a Deep Convolutional GAN (DCGAN) for generating synthetic microstructure images from the MICRO2D dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1032d33",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "### Dataset:\n",
    "\n",
    "MICRO2D dataset:\n",
    "\n",
    "87,379  2-phase microstructures\n",
    "\n",
    "The Microstructures are periodic and 256x256 pixels\n",
    "\n",
    "10 Classes - `['AngEllipse','GRF', 'NBSA', 'RandomEllipse', 'VoidSmall', 'VoidSmallBig',\n",
    "'VoronoiLarge', 'VoronoiMedium', 'VoronoiMediumSpaced', 'VoronoiSmall']`\n",
    "Model Configuration:\n",
    "\n",
    "### Generator:\n",
    "\n",
    "Input: 100-dimensional latent space (sampled from a Gaussian distribution).\n",
    "\n",
    "Architecture:\n",
    "\n",
    "Dense layer reshaped into a 16×16×256 tensor.\n",
    "\n",
    "Progressive upsampling using Conv2DTranspose layers, culminating in 256×256×1 outputs.\n",
    "\n",
    "Batch normalization layers to stabilize training dynamics.\n",
    "\n",
    "LeakyReLU activation for hidden layers, with Tanh activation at the output layer.\n",
    "\n",
    "### Discriminator:\n",
    "\n",
    "Input: 256×256×1256×256×1 images.\n",
    "\n",
    "Architecture:\n",
    "\n",
    "Conv2D layers with progressively increasing filters (32→512).\n",
    "\n",
    "Dropout layers for regularization.\n",
    "\n",
    "Batch normalization to ensure stable updates.\n",
    "\n",
    "Sigmoid activation for binary classification (real or fake).\n",
    "\n",
    "### Training Parameters:\n",
    "\n",
    "Latent dimension: 100\n",
    "\n",
    "Batch size: 32\n",
    "\n",
    "Learning rates:\n",
    "Generator: 0.0001\n",
    "\n",
    "Discriminator: 0.0002\n",
    "\n",
    "Optimizer: Adam with β1=0.5,β2=0.5\n",
    "\n",
    "### Evaluation Metrics:\n",
    "\n",
    "Generator loss (Binary cross-entropy).\n",
    "\n",
    "Discriminator loss (Binary cross-entropy).\n",
    "\n",
    "Visual inspection of generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88844aad",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "### Data Preparation:\n",
    "\n",
    "Images were loaded from the MICRO2D dataset using the h5py library.\n",
    "Normalized pixel values to `[−1,1][−1,1].`\n",
    "Added a channel dimension to ensure compatibility with TensorFlow models.\n",
    "\n",
    "### Model Implementation:\n",
    "\n",
    "Defined generator and discriminator models using the TensorFlow Keras API.\n",
    "Compiled the DCGAN model with Binary Crossentropy loss.\n",
    "\n",
    "### Training:\n",
    "\n",
    "Implemented label smoothing for real images to improve discriminator performance.\n",
    "\n",
    "Trained the discriminator and generator alternately within each step:\n",
    "\n",
    "Discriminator: Minimized loss by classifying real images as 0.9 (smoothing) and fake images as 0.0(with noise for robustness).\n",
    "\n",
    "Generator: Minimized loss by fooling the discriminator into classifying fake images as 1.0\n",
    "\n",
    "Monitored generated images at regular intervals using a custom callback.\n",
    "\n",
    "### Visualization:\n",
    "\n",
    "Generated synthetic images after each epoch for qualitative analysis.\n",
    "Displayed 5×5 grids of generated samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd934044",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "The Deep Convolutional GAN (DCGAN) has challenges in generating high-quality and realistic images especially when tackling complex tasks such as microstructure generation. DCGAN frequently yields fuzzy and lower resolution images, this can result in gradients disappearing and hinder the generator from capturing intricate details. Additionally, DCGAN is prone to mode collapse,\n",
    "limiting the generator to produce a limited set of images thus reducing diversity. Moreover,the model struggles to maintain consistency, in more complex image classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18037ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef35d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = './MICRO2D_homogenized.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b71679",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = h5py.File(file_1,'r')\n",
    "print(list(dataset_1.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2faa52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NBSA_images(file_path):\n",
    "    with h5py.File(file_path, 'r') as dataset:\n",
    "        if 'NBSA' in dataset:\n",
    "            group = dataset['NBSA']\n",
    "            keys = list(group.keys())\n",
    "            if len(keys) > 0:\n",
    "                dataset_name = keys[0]\n",
    "                images = group[dataset_name][:]\n",
    "                return images\n",
    "            else:\n",
    "                print(\"No dataset found in the 'NBSA' group.\")\n",
    "        else:\n",
    "            print(\"'NBSA' class not found in the dataset.\")\n",
    "    return None\n",
    "def normalize_images(images):\n",
    "    images = images.astype('float32')\n",
    "    images = images * 2 - 1\n",
    "    return images \n",
    "def plot_images(images, class_name):\n",
    "    num_images = min(10, images.shape[0])\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 4))\n",
    "    fig.suptitle(f\"Images for Class: {class_name}\", fontsize=16)\n",
    "    for i in range(num_images):\n",
    "        axes[i].imshow(images[i], cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBSA = load_NBSA_images(file_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(NBSA,'NBSA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8816d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBSA.shape,NBSA.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea8b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = normalize_images(NBSA)\n",
    "train_images.shape,train_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522fe9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.min(),train_images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM=100\n",
    "image_size = 256   \n",
    "channels = 1\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 1\n",
    "Z_DIM = 100\n",
    "LEARNING_RATE_D = 0.0002\n",
    "LEARNING_RATE_G = 0.0001\n",
    "ADAM_BETA_1 = 0.5\n",
    "ADAM_BETA_2 = 0.5\n",
    "NOISE_PARAM = 1 \n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    layers,\n",
    "    models,\n",
    "    callbacks,\n",
    "    losses,\n",
    "    utils,\n",
    "    metrics,\n",
    "    optimizers,\n",
    ")\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img,array_to_img\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7885b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(len(train_images)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(LATENT_DIM):\n",
    "    generator_input = layers.Input(shape=(LATENT_DIM,))\n",
    "    x = layers.Dense(16 * 16 * 256, use_bias=False)(generator_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Reshape((16, 16, 256))(x)\n",
    "    # First upsampling block\n",
    "    x = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    # Second upsampling block\n",
    "    x = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    # Second upsampling block\n",
    "    x = layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    # Third upsampling block to reach 256x256\n",
    "    x = layers.Conv2DTranspose(1, kernel_size=4, strides=2, padding=\"same\", use_bias=False, activation=\"tanh\")(x)\n",
    "    generator = models.Model(generator_input, x, name=\"Generator\")\n",
    "    return generator\n",
    "generator = create_generator(LATENT_DIM)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a1236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator(image_size, channels):\n",
    "    discriminator_input = layers.Input(shape=(image_size, image_size, channels))\n",
    "    x = layers.Conv2D(32, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(discriminator_input)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    discriminator_output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    discriminator = models.Model(discriminator_input, discriminator_output, name=\"Discriminator\")\n",
    "    return discriminator\n",
    "discriminator = create_discriminator(image_size, channels)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim):\n",
    "        super(DCGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.g_loss = Mean(name='g_loss')\n",
    "        self.d_loss = Mean(name='d_loss')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.g_loss, self.d_loss]\n",
    "\n",
    "    def compile(self, g_optimizer, d_optimizer, loss_fn):\n",
    "        super(DCGAN, self).compile()\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_noise = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Train Discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_real = self.discriminator(real_images, training=True)\n",
    "            real_labels = tf.ones_like(pred_real) * 0.9  # Label smoothing for real labels\n",
    "            d_loss_real = self.loss_fn(real_labels, pred_real)\n",
    "\n",
    "            fake_images = self.generator(random_noise, training=False)\n",
    "            pred_fake = self.discriminator(fake_images, training=True)\n",
    "            fake_labels = tf.zeros_like(pred_fake) + 0.05 * tf.random.uniform(tf.shape(pred_fake))  # Noisy fake labels\n",
    "            d_loss_fake = self.loss_fn(fake_labels, pred_fake)\n",
    "\n",
    "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "\n",
    "        d_gradients = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train Generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_noise, training=True)\n",
    "            pred_fake = self.discriminator(fake_images, training=True)\n",
    "            labels = tf.ones_like(pred_fake)  # Generator tries to get fake images labeled as real\n",
    "            g_loss = self.loss_fn(labels, pred_fake)\n",
    "\n",
    "        g_gradients = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss.update_state(d_loss)\n",
    "        self.g_loss.update_state(g_loss)\n",
    "\n",
    "        return {'d_loss': self.d_loss.result(), 'g_loss': self.g_loss.result()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cbdcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGANMonitor(callbacks.Callback):\n",
    "    def __init__(self, num_imgs=25, latent_dim=100):\n",
    "        self.num_imgs = num_imgs\n",
    "        self.latent_dim = latent_dim\n",
    "        self.noise = tf.random.normal([num_imgs, latent_dim])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        g_img = self.model.generator(self.noise, training=False)\n",
    "        g_img = (g_img * 127.5) + 127.5\n",
    "        g_img = tf.clip_by_value(g_img, 0, 255)\n",
    "        g_img = g_img.numpy().astype(\"uint8\")\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        for i in range(self.num_imgs):\n",
    "            plt.subplot(5, 5, i + 1)\n",
    "            if g_img.shape[-1] == 1:  \n",
    "                plt.imshow(g_img[i, :, :, 0], cmap=\"gray\")\n",
    "            else:  \n",
    "                plt.imshow(g_img[i])\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a7e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgan = DCGAN(generator,discriminator,latent_dim=LATENT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgan.compile(g_optimizer=Adam(learning_rate=LEARNING_RATE_G,beta_1=ADAM_BETA_1),d_optimizer=Adam(learning_rate=LEARNING_RATE_D,beta_1=ADAM_BETA_2),loss_fn=BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ed43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = dcgan.fit(train_dataset,epochs=1000,callbacks = [DCGANMonitor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ff4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle('Loss')\n",
    "plt.plot(hist.history['c_loss'],label = 'd_loss')\n",
    "plt.plot(hist.history['g_loss'],label='g_loss')\n",
    "plt.plot(hist.history['c_wass_loss'], label='w_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5270d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_plot_samples(generator, num_samples=100):\n",
    "    noise = tf.random.normal([num_samples, 256, 256, 1])\n",
    "    generated_images = generator(noise)\n",
    "    generated_images = (generated_images + 1) / 2  # Denormalize\n",
    "    \n",
    "    fig, axs = plt.subplots(10, 10, figsize=(20, 20))\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        # Assuming the output is a 1D array of size 25\n",
    "        img = generated_images[i].numpy().squeeze()\n",
    "        img = img.reshape((5, 5))  # Reshape to 5x5\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.savefig('final_generated_samples.png')\n",
    "    plt.close()\n",
    "\n",
    "# Call the function\n",
    "generate_and_plot_samples(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddbb9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from scipy.stats import entropy\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72198d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale_to_rgb(images):\n",
    "    return tf.image.grayscale_to_rgb(images)\n",
    "\n",
    "def inception_score(images, model, batch_size=50, splits=10, epsilon=1e-16):\n",
    "    preds = model.predict(images, batch_size=BATCH_SIZE)\n",
    "    scores = []\n",
    "    for i in range(splits):\n",
    "        part = preds[i * (len(preds) // splits): (i + 1) * (len(preds) // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores.append([])\n",
    "        for j in range(part.shape[0]):\n",
    "            pyx = part[j, :]\n",
    "            scores[-1].append(entropy(pyx, py))\n",
    "    \n",
    "    scores = np.exp(np.mean(scores))\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "# Load pre-trained Inception model\n",
    "inception_model = InceptionV3(include_top=True, weights='imagenet', input_shape=(299, 299, 3))\n",
    "\n",
    "# Generate images and prepare them for Inception model\n",
    "latent_dim = LATENT_DIM  # Make sure this matches your generator's input dimension\n",
    "generated_images = generate_images(generator, LATENT_DIM, num_samples=1000)\n",
    "generated_images = tf.image.resize(generated_images, (299, 299))\n",
    "generated_images = grayscale_to_rgb(generated_images)  # Convert to RGB\n",
    "generated_images = preprocess_input(generated_images * 255)  # Scale to [0, 255] and preprocess\n",
    "\n",
    "# Calculate Inception Score\n",
    "is_mean, is_std = inception_score(generated_images, inception_model)\n",
    "print(f\"Inception Score: {is_mean} ± {is_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    # Resize images to 299x299\n",
    "    images = tf.image.resize(images, (299, 299))\n",
    "    # Convert grayscale to RGB if necessary\n",
    "    if images.shape[-1] == 1:\n",
    "        images = tf.image.grayscale_to_rgb(images)\n",
    "    # Preprocess for InceptionV3\n",
    "    images = preprocess_input(images * 255.0)\n",
    "    return images\n",
    "\n",
    "def calculate_fid(train_images, generated_images, model):\n",
    "    def get_features(images):\n",
    "        features = model.predict(images)\n",
    "        return features\n",
    "\n",
    "    # Preprocess both real and generated images\n",
    "    real_images = preprocess_images(train_images)\n",
    "    generated_images = preprocess_images(generated_images)\n",
    "\n",
    "    real_features = get_features(real_images)\n",
    "    gen_features = get_features(generated_images)\n",
    "    \n",
    "    mu1, sigma1 = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = np.mean(gen_features, axis=0), np.cov(gen_features, rowvar=False)\n",
    "    \n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "# Load pre-trained Inception model\n",
    "inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n",
    "\n",
    "# Generate images\n",
    "latent_dim = LATENT_DIM  \n",
    "num_samples = 1000  \n",
    "generated_images = generate_images(generator, LATENT_DIM, num_samples=num_samples)\n",
    "\n",
    "# Calculate FID\n",
    "fid = calculate_fid(train_images[:num_samples], generated_images, inception_model)\n",
    "print(f\"Fréchet Inception Distance: {fid}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
