{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f713efd-3eeb-4bd0-948c-ff105f4b92e4",
   "metadata": {},
   "source": [
    "Lab Notebook: Hyperparameter tuning Diffusion Models for Microstructure Generation\n",
    "\n",
    "\n",
    "This notebook implements a hyperparameter tuning for diffusion model for generating synthetic microstructures. \n",
    "The key components include:\n",
    "\n",
    "Data Loading: Custom dataset loader for microstructure data from HDF5 files\n",
    "Model Architecture: UNet with self-attention and time conditioning\n",
    "Diffusion Process: Forward and reverse diffusion with DDIM sampling\n",
    "Evaluation: Domain-specific metrics for microstructure characterization\n",
    "Hyperparameter Tuning: Grid search and random search implementations\n",
    "\n",
    "Background\n",
    "Diffusion models have emerged as powerful generative models capable of producing high-quality, diverse samples. They work by gradually adding noise to data in a forward process, then learning to reverse this process to generate new samples. For microstructure generation, this approach is particularly promising as it can capture complex spatial correlations and phase distributions that are critical for material properties.\n",
    "The implementation focuses on nickel-based superalloys (NBSA), which have a characteristic two-phase microstructure that significantly influences their mechanical properties.\n",
    "Data Preparation\n",
    "The dataset consists of 2D microstructure images stored in an HDF5 file format. The custom Micro2DKeyDataset class handles:\n",
    "\n",
    "Loading specific microstructure classes from the HDF5 file\n",
    "Normalizing pixel values to the [-1, 1] range (standard for diffusion models)\n",
    "Optional data augmentation through random flips and affine transformations\n",
    "\n",
    "Data augmentation is used to increase diversity in the training set and improve model generalization.\n",
    "Model Architecture\n",
    "The model is built on a U-Net architecture, which is well-suited for image-to-image tasks:\n",
    "Key Components:\n",
    "\n",
    "Time Embedding: Sinusoidal position embeddings convert diffusion timesteps into high-dimensional vectors that condition the model.\n",
    "Down/Up Blocks: Specialized convolutional blocks that:\n",
    "\n",
    "Process spatial features at multiple resolutions\n",
    "Incorporate time information at each layer\n",
    "Use skip connections to preserve spatial details\n",
    "\n",
    "\n",
    "Self-Attention: Implemented at the middle of the U-Net to capture long-range dependencies in the microstructure, which is crucial for maintaining phase connectivity and morphology.\n",
    "Batch Normalization: Improves training stability and convergence.\n",
    "\n",
    "Diffusion Process\n",
    "The diffusion process is implemented in the DiffusionModel class:\n",
    "Forward Process\n",
    "\n",
    "Gradually adds Gaussian noise to images according to a predefined schedule\n",
    "Uses a linear beta schedule that controls the noise level at each timestep\n",
    "\n",
    "Reverse Process\n",
    "\n",
    "The U-Net model is trained to predict the noise added at each timestep\n",
    "During training, random timesteps are sampled to teach the model to denoise from any point in the process\n",
    "\n",
    "DDIM Sampling\n",
    "\n",
    "Implements Denoising Diffusion Implicit Models (DDIM) sampling for faster generation\n",
    "Uses a controllable stochasticity parameter (eta) to balance between deterministic and stochastic sampling\n",
    "Allows for generating high-quality samples with fewer steps than the original training process\n",
    "\n",
    "Evaluation Metrics\n",
    "Specialized evaluation metrics are implemented to assess the quality of generated microstructures:\n",
    "\n",
    "Structural Similarity Index (SSIM): Measures visual similarity between generated and real microstructures\n",
    "Two-Point Correlation Function (S2):\n",
    "\n",
    "Quantifies spatial correlations in the microstructure\n",
    "Implemented using FFT for computational efficiency\n",
    "Computes a normalized discrepancy between original and generated functions\n",
    "\n",
    "Lineal Path Function:\n",
    "\n",
    "Measures the probability of finding continuous line segments within a single phase\n",
    "Important for capturing phase connectivity that affects material properties\n",
    "Implemented using an efficient sampling approach\n",
    "\n",
    "These metrics provide a comprehensive assessment beyond visual similarity, focusing on structural characteristics relevant to material properties.\n",
    "Hyperparameter Tuning\n",
    "Two search approaches are implemented:\n",
    "Grid Search\n",
    "\n",
    "Systematically evaluates all combinations of hyperparameters\n",
    "Provides comprehensive coverage of the parameter space\n",
    "Computationally expensive for large parameter spaces\n",
    "\n",
    "Random Search\n",
    "\n",
    "Samples random configurations from the parameter space\n",
    "More efficient for high-dimensional spaces\n",
    "Handles different parameter types:\n",
    "\n",
    "Log-uniform sampling for learning rates\n",
    "Uniform sampling for continuous parameters\n",
    "Random selection from lists for categorical parameters\n",
    "\n",
    "Both methods include:\n",
    "\n",
    "Early stopping to prevent overfitting\n",
    "Automatic saving of models, configurations, and results\n",
    "Comprehensive logging and visualization of results\n",
    "\n",
    "Experimental Results\n",
    "For each hyperparameter configuration, the system:\n",
    "\n",
    "Trains the model for a specified number of epochs (with early stopping)\n",
    "Evaluates the model on validation data\n",
    "Generates sample microstructures\n",
    "Computes evaluation metrics\n",
    "Creates visualizations of training loss curves and generated samples\n",
    "\n",
    "Results are organized in a hierarchical directory structure with:\n",
    "\n",
    "JSON files containing configuration details and metrics\n",
    "PNG images of loss curves and generated samples\n",
    "Summary visualizations showing parameter impact on performance\n",
    "A ranking of the top parameter combinations\n",
    "\n",
    "Visualization Tools\n",
    "The implementation includes visualization functions for:\n",
    "\n",
    "Displaying batches of microstructures in a grid\n",
    "Plotting loss curves during training\n",
    "Creating parameter impact plots (validation loss vs. parameter value)\n",
    "Visualizing the correlation between different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27631e-0e32-40da-aa85-ff34e02b550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameter tuning for diffusion models.\n",
    "\"\"\"\n",
    "#Import the required libraries \n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm  # Progress bar utility\n",
    "from einops import rearrange  # Tensor reshaping utility\n",
    "from skimage.metrics import structural_similarity as ssim  # Image similarity metric\n",
    "import torchvision.transforms as transforms\n",
    "from datetime import datetime\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb5eea-63a3-44e9-b97d-91b9e79d780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a13475-cb2d-4f60-b45b-404cc3c6f57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class                          \n",
    "class Micro2DKeyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for loading 2D microstructure data from HDF5 files.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the HDF5 file\n",
    "        key (str): Key to access specific microstructure class in the HDF5 file\n",
    "        transform (callable, optional): Optional transform to be applied on a sample\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, key, transform=None):\n",
    "        self.file_path = file_path\n",
    "        self.key = key\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load data from HDF5 file\n",
    "        with h5py.File(self.file_path, 'r') as f:\n",
    "            self.data = f[key][key][:]\n",
    "        self.data = self.data.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image and normalize to [0, 1] range\n",
    "        img = self.data[idx]\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "        \n",
    "        # Convert to PyTorch tensor and add channel dimension\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        \n",
    "        # Scale to [-1, 1] range for diffusion model\n",
    "        img = 2 * img - 1\n",
    "        img = torch.clamp(img, -1.0, 1.0)  # Ensure values are strictly within range\n",
    "        \n",
    "        # Apply transforms if specified\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d2ffa-7281-4940-af8b-551ec3cc2fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet Model Components                  \n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal position embeddings for timestep encoding.\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Dimension of the embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        \"\"\"\n",
    "        Convert timesteps to sinusoidal embeddings.\n",
    "        \n",
    "        Args:\n",
    "            time (tensor): Tensor of timesteps [batch_size]\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Sinusoidal embeddings [batch_size, dim]\n",
    "        \"\"\"\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic UNet block with timestep conditioning.\n",
    "    \n",
    "    Args:\n",
    "        in_ch (int): Number of input channels\n",
    "        out_ch (int): Number of output channels\n",
    "        time_emb_dim (int): Dimension of time embedding\n",
    "        up (bool): Whether this is an upsampling block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        self.up = up\n",
    "        \n",
    "        if up:\n",
    "            # For upsampling blocks\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)  # Transposed conv for upsampling\n",
    "        else:\n",
    "            # For downsampling blocks\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)  # Stride 2 for downsampling\n",
    "            \n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Forward pass through the block.\n",
    "        \n",
    "        Args:\n",
    "            x (tensor): Input tensor [batch_size, in_ch, height, width]\n",
    "            t (tensor): Time embedding [batch_size, time_emb_dim]\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Output tensor after transformation\n",
    "        \"\"\"\n",
    "        # First Conv + BatchNorm + ReLU\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        \n",
    "        # Time embedding conditioning\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        \n",
    "        # Extend time embedding dimensions to match spatial dimensions\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]  # Add two dimensions\n",
    "        \n",
    "        # Add time channel to feature map\n",
    "        h = h + time_emb\n",
    "        \n",
    "        # Second Conv + BatchNorm + ReLU\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        \n",
    "        # Down or Upsample using transform layer\n",
    "        return self.transform(h)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention module for capturing long-range dependencies.\n",
    "    \n",
    "    Args:\n",
    "        channels (int): Number of input channels\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels        \n",
    "        self.mha = nn.MultiheadAttention(channels, 1, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for self-attention module.\n",
    "        \n",
    "        Args:\n",
    "            x (tensor): Input tensor [batch_size, channels, height, width]\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Self-attention output with same shape\n",
    "        \"\"\"\n",
    "        # Store original spatial dimensions\n",
    "        size = x.shape[-2:]\n",
    "        \n",
    "        # Rearrange tensor for attention operation:\n",
    "        # [batch, channels, height, width] -> [batch, height*width, channels]\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        \n",
    "        # Layer normalization\n",
    "        x_ln = self.ln(x)\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        \n",
    "        # Residual connection\n",
    "        attention_value = attention_value + x\n",
    "        \n",
    "        # Feed-forward network with residual connection\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        \n",
    "        # Rearrange back to original spatial dimensions:\n",
    "        # [batch, height*width, channels] -> [batch, channels, height, width]\n",
    "        return rearrange(attention_value, 'b (h w) c -> b c h w', h=size[0], w=size[1])\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet architecture for diffusion model with time conditioning and attention mechanism.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels (default: 1 for grayscale)\n",
    "        out_channels (int): Number of output channels (default: 1)\n",
    "        time_emb_dim (int): Dimension of time embedding (default: 128)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, out_channels=1, time_emb_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # For debugging\n",
    "        self.debug = False\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(in_channels, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Downsample blocks\n",
    "        self.down1 = Block(128, 128, time_emb_dim)   # 256x256 -> 128x128\n",
    "        self.down2 = Block(128, 256, time_emb_dim)   # 128x128 -> 64x64\n",
    "        self.down3 = Block(256, 256, time_emb_dim)   # 64x64 -> 32x32\n",
    "        self.down4 = Block(256, 512, time_emb_dim)   # 32x32 -> 16x16\n",
    "        self.down5 = Block(512, 512, time_emb_dim)   # 16x16 -> 8x8\n",
    "        \n",
    "        # Attention layer at the bottleneck (middle of U-Net)\n",
    "        self.attention = SelfAttention(256)\n",
    "        \n",
    "        # Upsampling blocks with skip connections\n",
    "        self.up1 = Block(512, 512, time_emb_dim, up=True)                # 8x8 -> 16x16\n",
    "        self.up2 = Block(512 + 512, 256, time_emb_dim, up=True)          # 16x16 -> 32x32\n",
    "        self.up3 = Block(256 + 256, 256, time_emb_dim, up=True)          # 32x32 -> 64x64\n",
    "        self.up4 = Block(256 + 256, 128, time_emb_dim, up=True)          # 64x64 -> 128x128\n",
    "        self.up5 = Block(128 + 128, 128, time_emb_dim, up=True)          # 128x128 -> 256x256\n",
    "        \n",
    "        # Final output projection\n",
    "        self.output = nn.Conv2d(128, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x, timestep):\n",
    "        \"\"\"\n",
    "        Forward pass through UNet.\n",
    "        \n",
    "        Args:\n",
    "            x (tensor): Input noisy image [batch_size, in_channels, height, width]\n",
    "            timestep (tensor): Diffusion timesteps [batch_size]\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Predicted noise [batch_size, out_channels, height, width]\n",
    "        \"\"\"\n",
    "        # Get time embedding\n",
    "        t = self.time_mlp(timestep)\n",
    "        \n",
    "        # Initial conv\n",
    "        x0 = self.conv0(x)  # 256x256\n",
    "        if self.debug:\n",
    "            print(f\"x0 (initial conv): {x0.shape}\")\n",
    "        \n",
    "        # Downsample path (encoder)\n",
    "        x1 = self.down1(x0, t)  # 128x128\n",
    "        if self.debug:\n",
    "            print(f\"x1 (down1): {x1.shape}\")\n",
    "            \n",
    "        x2 = self.down2(x1, t)  # 64x64\n",
    "        if self.debug:\n",
    "            print(f\"x2 (down2): {x2.shape}\")\n",
    "            \n",
    "        x3 = self.down3(x2, t)  # 32x32\n",
    "        x3 = self.attention(x3)  # Apply attention at bottleneck\n",
    "        if self.debug:\n",
    "            print(f\"x3 (down3 + attention): {x3.shape}\")\n",
    "            \n",
    "        x4 = self.down4(x3, t)  # 16x16\n",
    "        if self.debug:\n",
    "            print(f\"x4 (down4): {x4.shape}\")\n",
    "            \n",
    "        x5 = self.down5(x4, t)  # 8x8 (bottleneck)\n",
    "        if self.debug:\n",
    "            print(f\"x5 (down5): {x5.shape}\")\n",
    "        \n",
    "        # Upsample path (decoder) with skip connections\n",
    "        u1 = self.up1(x5, t)  # 16x16\n",
    "        if self.debug:\n",
    "            print(f\"u1 (up1): {u1.shape}\")\n",
    "            print(f\"x4 for concat: {x4.shape}\")\n",
    "            \n",
    "        # Concatenate with skip connections\n",
    "        u2 = self.up2(torch.cat([u1, x4], dim=1), t)  # 32x32\n",
    "        if self.debug:\n",
    "            print(f\"u2 (up2): {u2.shape}\")\n",
    "            print(f\"x3 for concat: {x3.shape}\")\n",
    "            \n",
    "        u3 = self.up3(torch.cat([u2, x3], dim=1), t)  # 64x64\n",
    "        if self.debug:\n",
    "            print(f\"u3 (up3): {u3.shape}\")\n",
    "            print(f\"x2 for concat: {x2.shape}\")\n",
    "            \n",
    "        u4 = self.up4(torch.cat([u3, x2], dim=1), t)  # 128x128\n",
    "        if self.debug:\n",
    "            print(f\"u4 (up4): {u4.shape}\")\n",
    "            print(f\"x1 for concat: {x1.shape}\")\n",
    "            \n",
    "        u5 = self.up5(torch.cat([u4, x1], dim=1), t)  # 256x256\n",
    "        if self.debug:\n",
    "            print(f\"u5 (up5): {u5.shape}\")\n",
    "        \n",
    "        # Final projection to output channels\n",
    "        output = self.output(u5)\n",
    "        if self.debug:\n",
    "            print(f\"output: {output.shape}\")\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1a058-e89f-4eff-a505-8d6646b29e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion Model Implementation         \n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    \"\"\"\n",
    "    Linear beta schedule for diffusion process.\n",
    "    \n",
    "    Args:\n",
    "        timesteps (int): Number of diffusion timesteps\n",
    "        start (float): Starting beta value\n",
    "        end (float): Ending beta value\n",
    "        \n",
    "    Returns:\n",
    "        tensor: Beta values schedule\n",
    "    \"\"\"\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "class DiffusionModel:\n",
    "    \"\"\"\n",
    "    Diffusion model implementation with forward and reverse processes.\n",
    "    \n",
    "    Args:\n",
    "        timesteps (int): Number of diffusion steps (default: 1000)\n",
    "        beta_schedule (str): Type of beta schedule (default: 'linear')\n",
    "        device (str): Device to run the model on (default: 'cuda')\n",
    "    \"\"\"\n",
    "    def __init__(self, timesteps=1000, beta_schedule='linear', device='cuda'):\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        # Define noise schedule\n",
    "        self.betas = linear_beta_schedule(timesteps).to(device)\n",
    "        \n",
    "        # Define alphas (1 - beta)\n",
    "        self.alphas = 1. - self.betas\n",
    "        \n",
    "        # Cumprod of alphas for q(x_t | x_0) calculations\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        # Previous alpha cumulative product for posterior variance calculation\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # Pre-compute values for diffusion process and sampling\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = torch.log(1. - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = torch.sqrt(1. / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1. / self.alphas_cumprod - 1)\n",
    "        \n",
    "        # Posterior variance calculation for q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "        \n",
    "    def q_sample(self, x_0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion process: Add noise to the original data.\n",
    "        \n",
    "        Args:\n",
    "            x_0 (tensor): Original clean image [batch_size, channels, height, width]\n",
    "            t (tensor): Timesteps [batch_size]\n",
    "            noise (tensor, optional): Noise to add, if None random noise is generated\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Noisy image at timestep t\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        \n",
    "        # Reshape coefficients for broadcasting\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].reshape(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].reshape(-1, 1, 1, 1)\n",
    "        \n",
    "        # Apply forward diffusion equation: x_t = sqrt(α_t)x_0 + sqrt(1-α_t)ε\n",
    "        return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    \n",
    "    def p_losses(self, denoise_model, x_0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Calculate loss for training the denoising model.\n",
    "        \n",
    "        Args:\n",
    "            denoise_model (nn.Module): The UNet model to predict noise\n",
    "            x_0 (tensor): Original clean images [batch_size, channels, height, width]\n",
    "            t (tensor): Timesteps [batch_size]\n",
    "            noise (tensor, optional): Noise to add, if None random noise is generated\n",
    "            \n",
    "        Returns:\n",
    "            tensor: MSE loss between predicted and actual noise\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        \n",
    "        # Add noise to input\n",
    "        x_noisy = self.q_sample(x_0, t, noise=noise)\n",
    "        \n",
    "        # Model predicts the noise added\n",
    "        predicted_noise = denoise_model(x_noisy, t)\n",
    "        \n",
    "        # Loss is MSE between predicted and actual noise\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def ddim_sample(self, model, shape, n_steps=150, eta=0.0):\n",
    "        \"\"\"\n",
    "        DDIM sampling for accelerated image generation.\n",
    "        \n",
    "        Args:\n",
    "            model (nn.Module): Trained UNet model\n",
    "            shape (tuple): Shape of samples to generate [batch_size, channels, height, width]\n",
    "            n_steps (int): Number of sampling steps (less than self.timesteps)\n",
    "            eta (float): Parameter controlling stochasticity (0 = deterministic, 1 = DDPM)\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Generated samples\n",
    "        \"\"\"\n",
    "        device = next(model.parameters()).device\n",
    "        b = shape[0]\n",
    "        \n",
    "        # Start from pure noise\n",
    "        img = torch.randn(shape, device=device)\n",
    "        \n",
    "        # Select subset of timesteps for DDIM sampling\n",
    "        timesteps = np.linspace(0, self.timesteps - 1, n_steps, dtype=int)[::-1]\n",
    "        \n",
    "        for i in tqdm(range(len(timesteps) - 1), desc='DDIM Sampling'):\n",
    "            t_current = torch.full((b,), timesteps[i], device=device, dtype=torch.long)\n",
    "            t_next = torch.full((b,), timesteps[i + 1], device=device, dtype=torch.long)\n",
    "            \n",
    "            # Predict noise\n",
    "            with torch.no_grad():\n",
    "                predicted_noise = model(img, t_current)\n",
    "            \n",
    "            # Extract x0 from xt using the predicted noise\n",
    "            alpha_cumprod_t = self.alphas_cumprod[t_current].reshape(-1, 1, 1, 1)\n",
    "            alpha_cumprod_next = self.alphas_cumprod[t_next].reshape(-1, 1, 1, 1)\n",
    "            \n",
    "            sqrt_alpha_cumprod_t = torch.sqrt(alpha_cumprod_t)\n",
    "            sqrt_one_minus_alpha_cumprod_t = torch.sqrt(1 - alpha_cumprod_t)\n",
    "            \n",
    "            # Predict x0 from xt and predicted noise\n",
    "            predicted_x0 = (img - sqrt_one_minus_alpha_cumprod_t * predicted_noise) / sqrt_alpha_cumprod_t\n",
    "            \n",
    "            # Calculate the coefficient for predicted_x0\n",
    "            sqrt_alpha_cumprod_next = torch.sqrt(alpha_cumprod_next)\n",
    "            \n",
    "            # Calculate the coefficient for direction pointing to xt\n",
    "            # Interpolate between DDPM and DDIM using eta\n",
    "            sigma_t = eta * torch.sqrt((1 - alpha_cumprod_next) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_next))\n",
    "            \n",
    "            # Get noise for stochastic part (if eta > 0)\n",
    "            noise = torch.randn_like(img) if eta > 0 else 0\n",
    "            \n",
    "            # Compute the next sample using DDIM formula\n",
    "            img = sqrt_alpha_cumprod_next * predicted_x0 + \\\n",
    "                  torch.sqrt(1 - alpha_cumprod_next - sigma_t**2) * predicted_noise + \\\n",
    "                  sigma_t * noise\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7fe1fd-1eaa-4482-928e-96a66a0d7651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics                     \n",
    "def two_point_correlation(image, max_distance=None):\n",
    "    \"\"\"\n",
    "    Compute two-point correlation function for a binary image.\n",
    "    This is a common metric for microstructure characterization.\n",
    "    \n",
    "    Args:\n",
    "        image (tensor or array): Binary image\n",
    "        max_distance (int, optional): Maximum distance to compute correlation\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (distances, correlation values)\n",
    "    \"\"\"\n",
    "    # Ensure binary image and convert to numpy\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.cpu().numpy()\n",
    "    \n",
    "    image = image.squeeze()\n",
    "    if image.max() > 1:\n",
    "        image = image / 255.0\n",
    "    image = (image > 0.5).astype(np.float32)\n",
    "    \n",
    "    h, w = image.shape\n",
    "    if max_distance is None:\n",
    "        max_distance = min(h, w) // 2\n",
    "    \n",
    "    # Calculate autocorrelation using FFT for efficiency\n",
    "    ft = np.fft.fft2(image)\n",
    "    power_spectrum = np.abs(np.fft.ifft2(ft * np.conj(ft)))**2\n",
    "    center = np.fft.fftshift(power_spectrum)[h//2, w//2]\n",
    "    \n",
    "    # Compute radial average (azimuthal integration)\n",
    "    y, x = np.indices((h, w))\n",
    "    r = np.sqrt((x - w//2)**2 + (y - h//2)**2).astype(np.int32)\n",
    "    r_max = min(h//2, w//2, max_distance)\n",
    "    \n",
    "    tbin = np.bincount(r.ravel(), power_spectrum.ravel())\n",
    "    nr = np.bincount(r.ravel())\n",
    "    radial_profile = tbin / nr\n",
    "    \n",
    "    # Normalize by central value\n",
    "    radial_profile = radial_profile[:r_max] / center\n",
    "    \n",
    "    return np.arange(r_max), radial_profile\n",
    "\n",
    "def lineal_path_function(image, max_distance=None):\n",
    "    \"\"\"\n",
    "    Compute lineal path function for a binary image.\n",
    "    This measures the probability of having a continuous line segment\n",
    "    completely inside one phase of the microstructure.\n",
    "    \n",
    "    Args:\n",
    "        image (tensor or array): Binary image\n",
    "        max_distance (int, optional): Maximum distance to compute lineal path\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (distances, lineal path values)\n",
    "    \"\"\"\n",
    "    # Ensure binary image and convert to numpy\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.cpu().numpy()\n",
    "    \n",
    "    image = image.squeeze()\n",
    "    if image.max() > 1:\n",
    "        image = image / 255.0\n",
    "    image = (image > 0.5).astype(np.float32)\n",
    "    \n",
    "    h, w = image.shape\n",
    "    if max_distance is None:\n",
    "        max_distance = min(h, w) // 4  # Limiting to 1/4 for computational efficiency\n",
    "    \n",
    "    # Initialize storage for lineal path function\n",
    "    lp = np.zeros(max_distance)\n",
    "    counts = np.zeros(max_distance)\n",
    "    \n",
    "    # Sample a subset of points for efficiency\n",
    "    n_samples = 1000\n",
    "    y_samples = np.random.randint(0, h, n_samples)\n",
    "    x_samples = np.random.randint(0, w, n_samples)\n",
    "    \n",
    "    # Check horizontal lines\n",
    "    for i, x in enumerate(x_samples):\n",
    "        y = y_samples[i]\n",
    "        if x + max_distance <= w:\n",
    "            line = image[y, x:x+max_distance]\n",
    "            for l in range(1, max_distance):\n",
    "                if np.all(line[:l+1] > 0.5):\n",
    "                    lp[l] += 1\n",
    "                counts[l] += 1\n",
    "    \n",
    "    # Check vertical lines\n",
    "    for i, y in enumerate(y_samples):\n",
    "        x = x_samples[i]\n",
    "        if y + max_distance <= h:\n",
    "            line = image[y:y+max_distance, x]\n",
    "            for l in range(1, max_distance):\n",
    "                if np.all(line[:l+1] > 0.5):\n",
    "                    lp[l] += 1\n",
    "                counts[l] += 1\n",
    "    \n",
    "    # Normalize by counts\n",
    "    lp = np.divide(lp, counts, out=np.zeros_like(lp), where=counts!=0)\n",
    "    \n",
    "    return np.arange(max_distance), lp\n",
    "\n",
    "def evaluate_microstructures(original_samples, generated_samples, max_distance=50):\n",
    "    \"\"\"\n",
    "    Evaluate generated microstructures against original ones using\n",
    "    multiple metrics: SSIM, two-point correlation, and lineal path function.\n",
    "    \n",
    "    Args:\n",
    "        original_samples (list or tensor): Original microstructure samples\n",
    "        generated_samples (list or tensor): Generated microstructure samples\n",
    "        max_distance (int): Maximum distance for correlation metrics\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'ssim_values': [],\n",
    "        's2_discrepancy': [],\n",
    "        'lineal_discrepancy': []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(original_samples)):\n",
    "        # Convert to numpy arrays in [0, 1] range\n",
    "        if isinstance(original_samples[i], torch.Tensor):\n",
    "            orig = ((original_samples[i] + 1) / 2).squeeze().cpu().numpy()\n",
    "        else:\n",
    "            orig = original_samples[i].squeeze()\n",
    "            \n",
    "        if isinstance(generated_samples[i], torch.Tensor):\n",
    "            gen = ((generated_samples[i] + 1) / 2).squeeze().cpu().numpy()\n",
    "        else:\n",
    "            gen = generated_samples[i].squeeze()\n",
    "        \n",
    "        # Compute SSIM (Structural Similarity Index)\n",
    "        ssim_val = ssim(orig, gen, data_range=1.0)\n",
    "        metrics['ssim_values'].append(ssim_val)\n",
    "        \n",
    "        # Compute two-point correlation function (S2)\n",
    "        r_orig, s2_orig = two_point_correlation(orig, max_distance)\n",
    "        r_gen, s2_gen = two_point_correlation(gen, max_distance)\n",
    "        \n",
    "        # Compute normalized area difference between curves\n",
    "        area_orig = np.trapz(s2_orig, r_orig)\n",
    "        area_between = np.trapz(np.abs(s2_orig - s2_gen), r_orig)\n",
    "        if area_orig > 0:\n",
    "            s2_discrepancy = (area_between / area_orig) * 100\n",
    "        else:\n",
    "            s2_discrepancy = 0\n",
    "        metrics['s2_discrepancy'].append(s2_discrepancy)\n",
    "        \n",
    "        # Compute lineal path function\n",
    "        r_orig, lp_orig = lineal_path_function(orig, max_distance)\n",
    "        r_gen, lp_gen = lineal_path_function(gen, max_distance)\n",
    "        \n",
    "        # Compute normalized area difference between curves\n",
    "        area_orig = np.trapz(lp_orig, r_orig)\n",
    "        area_between = np.trapz(np.abs(lp_orig - lp_gen), r_orig)\n",
    "        if area_orig > 0:\n",
    "            lineal_discrepancy = (area_between / area_orig) * 100\n",
    "        else:\n",
    "            lineal_discrepancy = 0\n",
    "        metrics['lineal_discrepancy'].append(lineal_discrepancy)\n",
    "    \n",
    "    # Compute averages for summary metrics\n",
    "    metrics['avg_ssim'] = np.mean(metrics['ssim_values'])\n",
    "    metrics['avg_s2_discrepancy'] = np.mean(metrics['s2_discrepancy'])\n",
    "    metrics['avg_lineal_discrepancy'] = np.mean(metrics['lineal_discrepancy'])\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b94c37-edfc-4a35-ab66-2a72a3fe0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions                \n",
    "def show_microstructure_batch(data, n=4, title=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Display a batch of microstructures in a grid.\n",
    "    \n",
    "    Args:\n",
    "        data (tensor or list): Batch of microstructure images\n",
    "        n (int): Number of images to display\n",
    "        title (str, optional): Title for the figure\n",
    "        save_path (str, optional): Path to save the figure\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(min(n, len(data))):\n",
    "        # Get image and convert to numpy\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            image = data[i].squeeze().cpu().detach().numpy()\n",
    "        else:\n",
    "            image = data[i].squeeze()\n",
    "            \n",
    "        # Convert from [-1, 1] to [0, 1] if needed\n",
    "        if image.min() < 0:\n",
    "            image = (image + 1) / 2\n",
    "            \n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "    if title:\n",
    "        plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6afa1-146a-4ec5-82ac-81705d449f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning Functions        \n",
    "# Convert NumPy types to standard Python types for JSON serialization\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"\n",
    "    Convert NumPy types to standard Python types for JSON serialization.\n",
    "    \n",
    "    Args:\n",
    "        obj: Object to convert (can be NumPy scalar, array, list, or dict)\n",
    "        \n",
    "    Returns:\n",
    "        Object with NumPy types converted to standard Python types\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64,\n",
    "                       np.uint8, np.uint16, np.uint32, np.uint64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.ndarray,)):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    return obj\n",
    "def simple_grid_search(param_grid, base_config, num_epochs=30):\n",
    "    \"\"\"\n",
    "    Simple grid search implementation for diffusion models.\n",
    "    \n",
    "    Args:\n",
    "        param_grid: Dictionary with parameter names as keys and lists of values to try\n",
    "        base_config: Base configuration with fixed parameters\n",
    "        num_epochs: Number of epochs to train each configuration\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with best parameters and all results\n",
    "    \"\"\"\n",
    "    # Create timestamp for saving results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir = os.path.join(base_config.get('save_dir', './'), f\"grid_search_{timestamp}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert param_grid values to standard Python types for JSON serialization\n",
    "    serializable_param_grid = convert_to_serializable(param_grid)\n",
    "    \n",
    "    # Log the parameter grid\n",
    "    with open(os.path.join(save_dir, 'param_grid.json'), 'w') as f:\n",
    "        json.dump(serializable_param_grid, f, indent=4)\n",
    "    \n",
    "    # Create all parameter combinations\n",
    "    keys = list(param_grid.keys())\n",
    "    values = list(param_grid.values())\n",
    "    param_combinations = list(itertools.product(*values))\n",
    "    \n",
    "    # Create the dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomAffine(degrees=10, scale=(0.9, 1.1)),\n",
    "    ]) if base_config.get('use_augmentation', True) else None\n",
    "    \n",
    "    dataset = Micro2DKeyDataset(\n",
    "        file_path=base_config['file_path'],\n",
    "        key=base_config['microstructure_class'],\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Create a validation split\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset, [train_size, val_size], \n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # Track results\n",
    "    all_results = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    \n",
    "    # Test each parameter combination\n",
    "    print(f\"Testing {len(param_combinations)} parameter combinations\")\n",
    "    \n",
    "    for i, combination in enumerate(param_combinations):\n",
    "        # Create parameter dictionary\n",
    "        params = dict(zip(keys, combination))\n",
    "        run_config = base_config.copy()\n",
    "        run_config.update(params)\n",
    "        \n",
    "        print(f\"\\nTesting combination {i+1}/{len(param_combinations)}:\")\n",
    "        for key, value in params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Create run directory\n",
    "        run_dir = os.path.join(save_dir, f\"run_{i+1}\")\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "        run_config['save_dir'] = run_dir\n",
    "        \n",
    "        # Save configuration\n",
    "        with open(os.path.join(run_dir, 'config.json'), 'w') as f:\n",
    "            # Convert any non-serializable objects or NumPy types to standard Python types\n",
    "            serializable_config = convert_to_serializable(run_config)\n",
    "            json.dump(serializable_config, f, indent=4)\n",
    "        \n",
    "        # Create model\n",
    "        model = UNet(\n",
    "            in_channels=1, \n",
    "            out_channels=1,\n",
    "            time_emb_dim=params.get('time_emb_dim', 128)\n",
    "        )\n",
    "        \n",
    "        # Create diffusion process\n",
    "        diffusion = DiffusionModel(\n",
    "            timesteps=params.get('timesteps', 1000),\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=params.get('learning_rate', 1e-4),\n",
    "            weight_decay=params.get('weight_decay', 1e-5)\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=params.get('batch_size', 8),\n",
    "            shuffle=True,\n",
    "            num_workers=run_config.get('num_workers', 2),\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=params.get('batch_size', 8),\n",
    "            shuffle=False,\n",
    "            num_workers=run_config.get('num_workers', 2),\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        best_epoch_val_loss = float('inf')\n",
    "        patience = 5  # Number of epochs with no improvement after which training will be stopped\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                batch = batch.to(device)\n",
    "                batch_size = batch.shape[0]\n",
    "                \n",
    "                # Sample random timesteps\n",
    "                t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device).long()\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = diffusion.p_losses(model, batch, t)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    batch = batch.to(device)\n",
    "                    batch_size = batch.shape[0]\n",
    "                    \n",
    "                    # Sample random timesteps\n",
    "                    t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device).long()\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = diffusion.p_losses(model, batch, t)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if avg_val_loss < best_epoch_val_loss:\n",
    "                best_epoch_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                \n",
    "                # Save best model for this run\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_epoch_val_loss\n",
    "                }, os.path.join(run_dir, \"best_model.pt\"))\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    break\n",
    "        \n",
    "        # Plot loss curves\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(run_dir, 'loss_curves.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Generate samples with the best model\n",
    "        checkpoint = torch.load(os.path.join(run_dir, \"best_model.pt\"))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Generate samples using DDIM sampling\n",
    "            samples = diffusion.ddim_sample(\n",
    "                model,\n",
    "                (4, 1, 256, 256),  # Generate 4 samples of 256x256 size\n",
    "                n_steps=params.get('sample_steps', 150),\n",
    "                eta=params.get('eta', 0.0)\n",
    "            )\n",
    "            \n",
    "            # Display and save samples\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            for i in range(4):\n",
    "                plt.subplot(2, 2, i+1)\n",
    "                plt.imshow(((samples[i] + 1) / 2).squeeze().cpu().numpy(), cmap='gray')\n",
    "                plt.title(f\"Sample {i+1}\")\n",
    "                plt.axis('off')\n",
    "            plt.suptitle(f\"Generated Samples\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(run_dir, \"samples.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # Calculate evaluation metrics on validation set\n",
    "        val_batch = next(iter(val_loader))[:4].to(device)\n",
    "        metrics = evaluate_microstructures(val_batch, samples)\n",
    "        \n",
    "        # Save metrics\n",
    "        with open(os.path.join(run_dir, 'metrics.json'), 'w') as f:\n",
    "            # Convert metrics to serializable format\n",
    "            serializable_metrics = convert_to_serializable(metrics)\n",
    "            json.dump(serializable_metrics, f, indent=4)\n",
    "        \n",
    "        # Record results\n",
    "        result = {\n",
    "            'params': convert_to_serializable(params),\n",
    "            'best_val_loss': best_epoch_val_loss,\n",
    "            'metrics': {\n",
    "                'ssim': float(metrics['avg_ssim']),\n",
    "                's2_discrepancy': float(metrics['avg_s2_discrepancy']),\n",
    "                'lineal_discrepancy': float(metrics['avg_lineal_discrepancy'])\n",
    "            }\n",
    "        }\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Update best parameters if this run has lower validation loss\n",
    "        if best_epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = best_epoch_val_loss\n",
    "            best_params = params.copy()\n",
    "            \n",
    "            # Save best model overall\n",
    "            checkpoint = torch.load(os.path.join(run_dir, \"best_model.pt\"))\n",
    "            torch.save(checkpoint, os.path.join(save_dir, \"best_model.pt\"))\n",
    "    \n",
    "    # Save all results\n",
    "    with open(os.path.join(save_dir, 'all_results.json'), 'w') as f:\n",
    "        json.dump(convert_to_serializable(all_results), f, indent=4)\n",
    "    \n",
    "    # Save best parameters\n",
    "    with open(os.path.join(save_dir, 'best_params.json'), 'w') as f:\n",
    "        json.dump(convert_to_serializable(best_params), f, indent=4)\n",
    "    \n",
    "    # Create results summary\n",
    "    create_results_summary(all_results, save_dir)\n",
    "    \n",
    "    return {\n",
    "        'best_params': best_params,\n",
    "        'all_results': all_results,\n",
    "        'save_dir': save_dir\n",
    "    }\n",
    "\n",
    "def create_results_summary(all_results, save_dir):\n",
    "    \"\"\"\n",
    "    Create summary visualizations of all hyperparameter search results.\n",
    "    \n",
    "    Args:\n",
    "        all_results (list): List of dictionaries containing results from each run\n",
    "        save_dir (str): Directory to save the summary visualizations\n",
    "    \"\"\"\n",
    "    # Extract data for plots\n",
    "    param_names = list(all_results[0]['params'].keys())\n",
    "    \n",
    "    # Create summaries for each parameter\n",
    "    for param_name in param_names:\n",
    "        values = []\n",
    "        val_losses = []\n",
    "        ssim_scores = []\n",
    "        \n",
    "        for result in all_results:\n",
    "            values.append(result['params'][param_name])\n",
    "            val_losses.append(result['best_val_loss'])\n",
    "            ssim_scores.append(result['metrics']['ssim'])\n",
    "        \n",
    "        # Create plots - convert to common data type\n",
    "        values = [float(v) if isinstance(v, (int, float)) else str(v) for v in values]\n",
    "        \n",
    "        # If we have numeric values, sort by them\n",
    "        if all(isinstance(v, (int, float)) for v in values):\n",
    "            # Sort all lists by parameter value\n",
    "            sorted_indices = np.argsort(values)\n",
    "            sorted_values = [values[i] for i in sorted_indices]\n",
    "            sorted_val_losses = [val_losses[i] for i in sorted_indices]\n",
    "            sorted_ssim_scores = [ssim_scores[i] for i in sorted_indices]\n",
    "            \n",
    "            # Plot val loss vs parameter\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(sorted_values, sorted_val_losses, 'o-')\n",
    "            plt.xlabel(param_name)\n",
    "            plt.ylabel('Validation Loss')\n",
    "            plt.title(f'Validation Loss vs {param_name}')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, f'{param_name}_val_loss.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot SSIM vs parameter\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(sorted_values, sorted_ssim_scores, 'o-')\n",
    "            plt.xlabel(param_name)\n",
    "            plt.ylabel('SSIM Score')\n",
    "            plt.title(f'SSIM Score vs {param_name}')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, f'{param_name}_ssim.png'))\n",
    "            plt.close()\n",
    "    \n",
    "    # Create table of top 5 best results\n",
    "    sorted_results = sorted(all_results, key=lambda x: x['best_val_loss'])\n",
    "    top5 = sorted_results[:5]\n",
    "    \n",
    "    with open(os.path.join(save_dir, 'top5_results.txt'), 'w') as f:\n",
    "        f.write(\"Top 5 Best Parameter Combinations:\\n\\n\")\n",
    "        for i, result in enumerate(top5):\n",
    "            f.write(f\"Rank {i+1}:\\n\")\n",
    "            f.write(f\"  Validation Loss: {result['best_val_loss']:.6f}\\n\")\n",
    "            f.write(f\"  SSIM: {result['metrics']['ssim']:.4f}\\n\")\n",
    "            f.write(f\"  S2 Discrepancy: {result['metrics']['s2_discrepancy']:.2f}%\\n\")\n",
    "            f.write(f\"  Lineal Discrepancy: {result['metrics']['lineal_discrepancy']:.2f}%\\n\")\n",
    "            f.write(\"  Parameters:\\n\")\n",
    "            for key, value in result['params'].items():\n",
    "                f.write(f\"    {key}: {value}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def random_search(param_ranges, base_config, num_trials=20, num_epochs=30):\n",
    "    \"\"\"\n",
    "    Random search implementation for diffusion models.\n",
    "    \n",
    "    Args:\n",
    "        param_ranges: Dictionary with parameter names as keys and (min, max) or list of values\n",
    "        base_config: Base configuration with fixed parameters\n",
    "        num_trials: Number of random configurations to try\n",
    "        num_epochs: Number of epochs to train each configuration\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with best parameters and all results\n",
    "    \"\"\"\n",
    "    # Generate random parameter grid\n",
    "    param_grid = {}\n",
    "    \n",
    "    for param_name, param_range in param_ranges.items():\n",
    "        if isinstance(param_range, tuple) and len(param_range) == 2:\n",
    "            # Continuous parameter range (min, max)\n",
    "            min_val, max_val = param_range\n",
    "            \n",
    "            # Handle different parameter types\n",
    "            if param_name == 'learning_rate' or param_name == 'weight_decay':\n",
    "                # Log-uniform sampling for learning rates\n",
    "                log_min = np.log(min_val)\n",
    "                log_max = np.log(max_val)\n",
    "                values = np.exp(np.random.uniform(log_min, log_max, num_trials))\n",
    "            elif isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                # Integer parameter\n",
    "                values = np.random.randint(min_val, max_val + 1, num_trials)\n",
    "            else:\n",
    "                # Uniform sampling for other parameters\n",
    "                values = np.random.uniform(min_val, max_val, num_trials)\n",
    "                \n",
    "        elif isinstance(param_range, list):\n",
    "            # Discrete choices\n",
    "            values = [np.random.choice(param_range) for _ in range(num_trials)]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported parameter range type for {param_name}: {type(param_range)}\")\n",
    "            \n",
    "        param_grid[param_name] = values\n",
    "    \n",
    "    # Convert param_grid from trial-based to parameter-based for grid search\n",
    "    transposed_grid = defaultdict(list)\n",
    "    \n",
    "    for trial in range(num_trials):\n",
    "        for param_name, values in param_grid.items():\n",
    "            transposed_grid[param_name].append(values[trial])\n",
    "    \n",
    "    # Run the search using grid search with the generated parameters\n",
    "    return simple_grid_search(dict(transposed_grid), base_config, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f729c-0289-4866-9608-e593cefea193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution                         \n",
    "if __name__ == \"__main__\":\n",
    "    # Define parameter grid for grid search\n",
    "    param_grid = {\n",
    "        'timesteps': [500, 1000, 1500],          # Number of diffusion steps\n",
    "        'sample_steps': [100, 150, 200],         # Number of sampling steps (DDIM)\n",
    "        'learning_rate': [1e-4, 3e-4, 5e-4],     # Learning rate for optimizer\n",
    "        'batch_size': [4, 8, 16],                # Batch size for training\n",
    "        'time_emb_dim': [64, 128, 256],          # Time embedding dimension\n",
    "        'eta': [0.0, 0.2, 0.5]                   # DDIM stochasticity parameter\n",
    "    }\n",
    "    \n",
    "    # Define parameter ranges for random search\n",
    "    param_ranges = {\n",
    "        'timesteps': (500, 2000),               # Continuous range of diffusion steps\n",
    "        'sample_steps': (100, 250),             # Continuous range of sampling steps\n",
    "        'learning_rate': (1e-5, 5e-4),          # Log-uniform range for learning rate\n",
    "        'batch_size': [4, 8, 16],               # Discrete options for batch size\n",
    "        'time_emb_dim': [64, 128, 256],         # Discrete options for embedding dimension\n",
    "        'weight_decay': (1e-6, 1e-4),           # Log-uniform range for weight decay\n",
    "        'eta': (0.0, 0.5)                       # Uniform range for DDIM stochasticity\n",
    "    }\n",
    "    \n",
    "    # Base configuration with fixed parameters\n",
    "    base_config = {\n",
    "        'file_path': '/lustre/uschill-lab/users/3782/diffusion/New_Diff/MICRO2D_homogenized.h5',\n",
    "        'microstructure_class': 'NBSA',         # Type of microstructure to model\n",
    "        'save_dir': f'./tuning_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "        'use_augmentation': True,               # Whether to use data augmentation\n",
    "        'num_workers': 2                        # Number of workers for data loading\n",
    "    }\n",
    "    \n",
    "    # Choose search method\n",
    "    search_method = \"random\"  # \"grid\" or \"random\"\n",
    "    \n",
    "    if search_method == \"grid\":\n",
    "        print(\"Running grid search...\")\n",
    "        results = simple_grid_search(param_grid, base_config, num_epochs=30)\n",
    "    else:\n",
    "        print(\"Running random search...\")\n",
    "        results = random_search(param_ranges, base_config, num_trials=20, num_epochs=30)\n",
    "    \n",
    "    print(\"\\nSearch completed!\")\n",
    "    print(\"Best parameters found:\")\n",
    "    for param, value in results['best_params'].items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    print(f\"Results saved to: {results['save_dir']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
