{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1080a1ab-af56-41bb-99f0-7e2e7261d67e",
   "metadata": {},
   "source": [
    "Lab Notebook: Diffusion Models for Microstructure Generation Class - VoidSmall\n",
    "\n",
    "This notebook implements a diffusion model pipeline for generating synthetic material microstructures. The implementation includes a complete workflow from data loading to evaluation.\n",
    "\n",
    "Technical Background\n",
    "\n",
    "Diffusion Models\n",
    "\n",
    "Diffusion models work through a forward process that gradually adds noise to data, followed by a reverse denoising process that reconstructs the data. The model is trained to predict the noise added at each step, enabling it to generate new samples starting from pure noise.\n",
    "\n",
    "Key features of the implementation:\n",
    "\n",
    "Forward diffusion process with linear beta schedule\n",
    "Reverse denoising using a UNet architecture with self-attention\n",
    "DDIM (Denoising Diffusion Implicit Models) sampling for faster generation\n",
    "\n",
    "Microstructure Characterization\n",
    "Microstructures are characterized using specialized metrics:\n",
    "\n",
    "Two-point correlation function (S2): Measures spatial correlations\n",
    "Lineal path function: Captures phase connectivity in the material\n",
    "Structural Similarity Index (SSIM): Quantifies visual similarity\n",
    "\n",
    "These metrics provide insights beyond visual comparison and relate directly to material properties.\n",
    "Implementation Details\n",
    "Dataset\n",
    "The Micro2DKeyDataset class handles loading microstructure data from HDF5 files:\n",
    "\n",
    "Normalizes pixel values to [-1, 1] range\n",
    "Applies data augmentation including flips and affine transformations\n",
    "Enforces value clamping to maintain numerical stability\n",
    "\n",
    "Model Architecture\n",
    "The UNet architecture includes:\n",
    "\n",
    "Time embedding via sinusoidal position embeddings\n",
    "Downsample path with 5 blocks (128 → 512 channels)\n",
    "Self-attention module at the middle of the network\n",
    "Upsample path with skip connections\n",
    "Batch normalization at each layer for training stability\n",
    "\n",
    "The self-attention mechanism is crucial for capturing long-range dependencies in microstructure patterns.\n",
    "Diffusion Process\n",
    "The DiffusionModel class implements:\n",
    "\n",
    "Forward diffusion with a linear noise schedule\n",
    "Loss calculation for noise prediction\n",
    "DDIM sampling with controllable stochasticity parameter (η)\n",
    "\n",
    "DDIM sampling accelerates generation by reducing the number of sampling steps while maintaining quality.\n",
    "Training Pipeline\n",
    "The train_diffusion_model function manages the training loop:\n",
    "\n",
    "Randomly samples timesteps during training\n",
    "Calculates loss by comparing predicted and actual noise\n",
    "Periodically saves model checkpoints and generates samples\n",
    "Visualizes progress throughout training\n",
    "\n",
    "Evaluation Metrics\n",
    "\n",
    "Multiple evaluation approaches are implemented:\n",
    "\n",
    "Material-specific metrics:\n",
    "\n",
    "Two-point correlation function discrepancy\n",
    "Lineal path function discrepancy\n",
    "SSIM for visual similarity\n",
    "\n",
    "\n",
    "General-purpose metrics:\n",
    "\n",
    "Fréchet Inception Distance (FID): Measures distribution similarity\n",
    "Inception Score (IS): Assesses quality and diversity of generated samples\n",
    "\n",
    "\n",
    "\n",
    "Experimental Setup\n",
    "\n",
    "Configuration\n",
    "\n",
    "The code uses a comprehensive configuration system with the following key parameters:\n",
    "config = {\n",
    "    'file_path': Path to HDF5 data file,\n",
    "    'microstructure_class': 'VoidSmall',  # Target microstructure class\n",
    "    'batch_size': 16,\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 5e-5,\n",
    "    'weight_decay': 1e-5,\n",
    "    'timesteps': 2000,  # Number of diffusion steps\n",
    "    'sample_steps': 150,  # Steps for DDIM sampling\n",
    "    'use_augmentation': True,\n",
    "    'num_workers': 2,\n",
    "    'save_dir': Path for saving results\n",
    "}\n",
    "Hardware and Environment\n",
    "\n",
    "Uses PyTorch with CUDA acceleration when available\n",
    "Leverages data parallelism through the DataLoader with multiple workers\n",
    "Employs tqdm for progress tracking during training and sampling\n",
    "\n",
    "Improvements and Robust Implementation\n",
    "Several robustness improvements are included:\n",
    "\n",
    "Error handling in metric calculation:\n",
    "\n",
    "Safely handles edge cases in two-point correlation function\n",
    "Uses numerical stability techniques (e.g., epsilon values for division)\n",
    "Returns fallback values instead of raising exceptions\n",
    "\n",
    "\n",
    "Inception metrics:\n",
    "\n",
    "Handles grayscale to RGB conversion for the inception model\n",
    "Processes images in batches to prevent memory issues\n",
    "Robust feature extraction for FID calculation\n",
    "\n",
    "\n",
    "Visualization safety:\n",
    "\n",
    "Ensures proper normalization for visualization\n",
    "Handles tensor to numpy conversion\n",
    "Provides options for both display and saving\n",
    "\n",
    "Usage Guide\n",
    "To train a new diffusion model:\n",
    "\n",
    "Configure the parameters in the config dictionary\n",
    "\n",
    "Call the run_training function\n",
    "\n",
    "Examine the results in the specified save directory:\n",
    "\n",
    "Saved model weights\n",
    "\n",
    "Generated samples\n",
    "\n",
    "Evaluation metrics and visualizations\n",
    "\n",
    "For inference with a trained model:\n",
    "\n",
    "Load the model state using torch.load\n",
    "\n",
    "Use the ddim_sample method of the diffusion model to generate samples\n",
    "\n",
    "Apply the evaluate_microstructures function to assess quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3f767-6dee-413c-8aac-fd490b37c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Diffusion Model for Microstructure Generation - VoidSmall\n",
    "\n",
    "This implementation creates a complete diffusion model pipeline for generating\n",
    "synthetic material microstructures. The model uses a UNet with self-attention\n",
    "to reverse a noise diffusion process, enabling high-quality generation of material\n",
    "structures with physically relevant properties.\n",
    "\n",
    "Key components:\n",
    "1. Custom dataset loading for microstructure data from HDF5 files\n",
    "2. UNet architecture with timestep conditioning and self-attention\n",
    "3. Diffusion model with forward and reverse processes\n",
    "4. Material-specific evaluation metrics (two-point correlation, lineal path)\n",
    "5. Training and visualization utilities\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from einops import rearrange  # For tensor reshaping operations\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm  \n",
    "from skimage.metrics import structural_similarity as ssim  # For image similarity assessment\n",
    "import torchvision.models as models  # For Inception model (FID/IS metrics)\n",
    "from scipy import linalg  # For matrix operations in FID calculation\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "from scipy import integrate  # For numerical integration in evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6477dad-931c-48f2-b21d-eb01766062e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd58203f-0be6-4a52-be3a-886a0f39cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Implementation \n",
    "class Micro2DKeyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for loading 2D microstructure data from HDF5 files.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the HDF5 data file\n",
    "        key (str): Specific microstructure class to load from the file\n",
    "        transform (callable, optional): Optional transformations to apply\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, key, transform=None):\n",
    "        self.file_path = file_path\n",
    "        self.key = key\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load data from HDF5 file\n",
    "        with h5py.File(self.file_path, 'r') as f:\n",
    "            self.data = f[key][key][:]\n",
    "        self.data = self.data.astype(np.float32)  # Convert to float32 for PyTorch\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples in the dataset\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a microstructure sample by index\n",
    "        \n",
    "        Processing steps:\n",
    "        1. Normalize the image to [0,1] range\n",
    "        2. Convert to PyTorch tensor\n",
    "        3. Scale to [-1,1] range (standard for diffusion models)\n",
    "        4. Apply any specified transformations\n",
    "        \"\"\"\n",
    "        img = self.data[idx]\n",
    "        # Normalize to [0,1] range with small epsilon to prevent division by zero\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "        # Convert to tensor and add channel dimension\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        # Scale to [-1,1] range\n",
    "        img = 2 * img - 1\n",
    "        # Ensure values are strictly within range\n",
    "        img = torch.clamp(img, -1.0, 1.0)\n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88631f2-2cd5-430e-9023-b874f6ac7444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions \n",
    "def show_microstructure_batch(data, n=4, title=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Display or save a batch of microstructure images in a grid.\n",
    "    \n",
    "    Args:\n",
    "        data (torch.Tensor or list): Batch of microstructure images\n",
    "        n (int): Number of images to display (max 4)\n",
    "        title (str, optional): Title for the figure\n",
    "        save_path (str, optional): Path to save the figure instead of displaying\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(min(n, len(data))):\n",
    "        # Get image and convert to numpy\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            image = data[i].squeeze().cpu().detach().numpy()\n",
    "        else:\n",
    "            image = data[i].squeeze()\n",
    "            \n",
    "        # Convert from [-1, 1] to [0, 1] if needed\n",
    "        if image.min() < 0:\n",
    "            image = (image + 1) / 2\n",
    "            \n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "    if title:\n",
    "        plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def print_debug(message):\n",
    "    \"\"\"Utility function for debug printing with a consistent format\"\"\"\n",
    "    print(f\"DEBUG: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d992325-5d08-44aa-b6cd-9b82066aa299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet Model Components \n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional embeddings for timestep conditioning in the diffusion model.\n",
    "    \n",
    "    Uses sinusoidal embeddings similar to those in the Transformer architecture\n",
    "    to convert scalar timesteps into high-dimensional feature vectors.\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Dimension of the embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        \"\"\"\n",
    "        Convert timesteps to sinusoidal embeddings\n",
    "        \n",
    "        Args:\n",
    "            time (tensor): Tensor of timesteps [batch_size]\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Sinusoidal embeddings [batch_size, dim]\n",
    "        \"\"\"\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        # Create log-spaced frequency bands\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        # Create embeddings through sinusoidal functions\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic UNet block with time conditioning for diffusion model.\n",
    "    \n",
    "    This block performs either downsampling or upsampling based on the 'up' parameter.\n",
    "    Each block includes convolutions, batch normalization, and time embedding injection.\n",
    "    \n",
    "    Args:\n",
    "        in_ch (int): Number of input channels\n",
    "        out_ch (int): Number of output channels\n",
    "        time_emb_dim (int): Dimension of time embedding\n",
    "        up (bool): Whether this is an upsampling block (False = downsampling)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        # Time embedding projection\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        self.up = up\n",
    "        \n",
    "        if up:\n",
    "            # For upsampling blocks\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            # Transposed convolution for upsampling (stride 2)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            # For downsampling blocks\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            # Strided convolution for downsampling (stride 2)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "            \n",
    "        # Second convolution after time conditioning\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        # Batch normalization for training stability\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Forward pass through the block\n",
    "        \n",
    "        Args:\n",
    "            x (tensor): Input feature map [B, in_ch, H, W]\n",
    "            t (tensor): Time embedding [B, time_emb_dim]\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Output feature map [B, out_ch, H/2, W/2] for down blocks\n",
    "                   or [B, out_ch, H*2, W*2] for up blocks\n",
    "        \"\"\"\n",
    "        # First Conv + BatchNorm + ReLU\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        \n",
    "        # Project time embedding to channel dimension\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        \n",
    "        # Extend time embedding dimensions to match spatial dimensions (add H, W dims)\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        \n",
    "        # Add time embedding to feature map (additive conditioning)\n",
    "        h = h + time_emb\n",
    "        \n",
    "        # Second Conv + BatchNorm + ReLU\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        \n",
    "        # Down or Upsample using transform layer\n",
    "        return self.transform(h)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention module for capturing long-range dependencies in images.\n",
    "    \n",
    "    Implements a form of self-attention similar to the one in the Transformer\n",
    "    architecture, adapted for 2D feature maps.\n",
    "    \n",
    "    Args:\n",
    "        channels (int): Number of input channels\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels        \n",
    "        # Multi-head attention with one head\n",
    "        self.mha = nn.MultiheadAttention(channels, 1, batch_first=True)\n",
    "        # Layer normalization\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        # Feed-forward network after attention\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),  # GELU activation for better gradient properties\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply self-attention to input feature map\n",
    "        \n",
    "        Args:\n",
    "            x (tensor): Input feature map [B, C, H, W]\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Output feature map with same shape but enhanced with attention\n",
    "        \"\"\"\n",
    "        # Store original spatial dimensions\n",
    "        size = x.shape[-2:]\n",
    "        \n",
    "        # Reshape tensor for attention operation: [B, C, H, W] -> [B, H*W, C]\n",
    "        # This treats each pixel as a \"token\" with C-dimensional features\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        x_ln = self.ln(x)\n",
    "        \n",
    "        # Apply multi-head self-attention\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        \n",
    "        # First residual connection\n",
    "        attention_value = attention_value + x\n",
    "        \n",
    "        # Feed-forward network with second residual connection\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        \n",
    "        # Reshape back to original spatial dimensions: [B, H*W, C] -> [B, C, H, W]\n",
    "        return rearrange(attention_value, 'b (h w) c -> b c h w', h=size[0], w=size[1])\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet architecture for diffusion model with time conditioning and self-attention.\n",
    "    \n",
    "    The UNet follows a standard encoder-decoder structure with skip connections,\n",
    "    but includes timestep conditioning and self-attention mechanisms which are\n",
    "    crucial for diffusion models.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input image channels (default=1 for grayscale)\n",
    "        out_channels (int): Number of output channels (default=1)\n",
    "        time_emb_dim (int): Dimension of time embedding (default=128)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, out_channels=1, time_emb_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # For debugging feature map dimensions\n",
    "        self.debug = False\n",
    "        \n",
    "        # Time embedding with sinusoidal positional encoding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Initial projection from image to feature map\n",
    "        self.conv0 = nn.Conv2d(in_channels, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Downsample path (encoder)\n",
    "        self.down1 = Block(128, 128, time_emb_dim)      # 256x256 -> 128x128\n",
    "        self.down2 = Block(128, 256, time_emb_dim)      # 128x128 -> 64x64\n",
    "        self.down3 = Block(256, 256, time_emb_dim)      # 64x64 -> 32x32\n",
    "        self.down4 = Block(256, 512, time_emb_dim)      # 32x32 -> 16x16\n",
    "        self.down5 = Block(512, 512, time_emb_dim)      # 16x16 -> 8x8\n",
    "        \n",
    "        # Self-attention module at the middle of the U-Net\n",
    "        # This helps capture long-range dependencies, which is crucial for\n",
    "        # maintaining global structure in the generated microstructures\n",
    "        self.attention = SelfAttention(256)\n",
    "        \n",
    "        # Upsample path (decoder) with skip connections\n",
    "        self.up1 = Block(512, 512, time_emb_dim, up=True)               # 8x8 -> 16x16\n",
    "        self.up2 = Block(512 + 512, 256, time_emb_dim, up=True)         # 16x16 -> 32x32\n",
    "        self.up3 = Block(256 + 256, 256, time_emb_dim, up=True)         # 32x32 -> 64x64\n",
    "        self.up4 = Block(256 + 256, 128, time_emb_dim, up=True)         # 64x64 -> 128x128\n",
    "        self.up5 = Block(128 + 128, 128, time_emb_dim, up=True)         # 128x128 -> 256x256\n",
    "        \n",
    "        # Final 1x1 convolution to predicted noise\n",
    "        self.output = nn.Conv2d(128, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x, timestep):\n",
    "        \"\"\"\n",
    "        Forward pass through UNet with skip connections\n",
    "        \n",
    "        Args:\n",
    "            x (tensor): Input noisy image [B, C, H, W]\n",
    "            timestep (tensor): Diffusion timesteps [B]\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Predicted noise [B, C, H, W]\n",
    "        \"\"\"\n",
    "        # Get time embedding\n",
    "        t = self.time_mlp(timestep)\n",
    "        \n",
    "        # Initial convolution\n",
    "        x0 = self.conv0(x)  # 256x256\n",
    "        if self.debug:\n",
    "            print(f\"x0 (initial conv): {x0.shape}\")\n",
    "        \n",
    "        # Downsample path (encoder) with debugging prints\n",
    "        x1 = self.down1(x0, t)  # 128x128\n",
    "        if self.debug:\n",
    "            print(f\"x1 (down1): {x1.shape}\")\n",
    "            \n",
    "        x2 = self.down2(x1, t)  # 64x64\n",
    "        if self.debug:\n",
    "            print(f\"x2 (down2): {x2.shape}\")\n",
    "            \n",
    "        x3 = self.down3(x2, t)  # 32x32\n",
    "        # Apply self-attention at the bottleneck of the UNet\n",
    "        x3 = self.attention(x3)\n",
    "        if self.debug:\n",
    "            print(f\"x3 (down3 + attention): {x3.shape}\")\n",
    "            \n",
    "        x4 = self.down4(x3, t)  # 16x16\n",
    "        if self.debug:\n",
    "            print(f\"x4 (down4): {x4.shape}\")\n",
    "            \n",
    "        x5 = self.down5(x4, t)  # 8x8 (bottleneck)\n",
    "        if self.debug:\n",
    "            print(f\"x5 (down5): {x5.shape}\")\n",
    "        \n",
    "        # Upsample path (decoder) with skip connections\n",
    "        u1 = self.up1(x5, t)  # 16x16\n",
    "        if self.debug:\n",
    "            print(f\"u1 (up1): {u1.shape}\")\n",
    "            print(f\"x4 for concat: {x4.shape}\")\n",
    "            \n",
    "        # Concatenate with skip connections\n",
    "        u2 = self.up2(torch.cat([u1, x4], dim=1), t)  # 32x32\n",
    "        if self.debug:\n",
    "            print(f\"u2 (up2): {u2.shape}\")\n",
    "            print(f\"x3 for concat: {x3.shape}\")\n",
    "            \n",
    "        u3 = self.up3(torch.cat([u2, x3], dim=1), t)  # 64x64\n",
    "        if self.debug:\n",
    "            print(f\"u3 (up3): {u3.shape}\")\n",
    "            print(f\"x2 for concat: {x2.shape}\")\n",
    "            \n",
    "        u4 = self.up4(torch.cat([u3, x2], dim=1), t)  # 128x128\n",
    "        if self.debug:\n",
    "            print(f\"u4 (up4): {u4.shape}\")\n",
    "            print(f\"x1 for concat: {x1.shape}\")\n",
    "            \n",
    "        u5 = self.up5(torch.cat([u4, x1], dim=1), t)  # 256x256\n",
    "        if self.debug:\n",
    "            print(f\"u5 (up5): {u5.shape}\")\n",
    "        \n",
    "        # Final 1x1 convolution to predicted noise\n",
    "        output = self.output(u5)\n",
    "        if self.debug:\n",
    "            print(f\"output: {output.shape}\")\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109164d-1706-476b-95ab-09e06722c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion Model Implementation \n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    \"\"\"\n",
    "    Linear noise schedule for diffusion process.\n",
    "    \n",
    "    Creates a linear schedule for beta values from start to end.\n",
    "    Beta controls the noise level added at each timestep.\n",
    "    \n",
    "    Args:\n",
    "        timesteps (int): Number of diffusion timesteps\n",
    "        start (float): Starting beta value (small)\n",
    "        end (float): Ending beta value (larger)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Linear schedule of beta values\n",
    "    \"\"\"\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "class DiffusionModel:\n",
    "    \"\"\"\n",
    "    Implements the diffusion process for image generation.\n",
    "    \n",
    "    This class handles both the forward process (adding noise) and provides\n",
    "    methods for the reverse process (removing noise) via sampling.\n",
    "    \n",
    "    Args:\n",
    "        timesteps (int): Number of diffusion steps (default=4000)\n",
    "        beta_schedule (str): Type of beta schedule (only 'linear' supported)\n",
    "        device (str): Device to run the model on (default='cuda')\n",
    "    \"\"\"\n",
    "    def __init__(self, timesteps=4000, beta_schedule='linear', device='cuda'):\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        # Define noise schedule (variance of noise added at each step)\n",
    "        self.betas = linear_beta_schedule(timesteps).to(device)\n",
    "        \n",
    "        # Define alphas = 1 - betas (signal proportion kept at each step)\n",
    "        self.alphas = 1. - self.betas\n",
    "        \n",
    "        # Cumulative product of alphas (for computing statistics of q(x_t | x_0))\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        # Previous alpha cumulative product (for posterior variance calculation)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # Pre-compute values for diffusion process and sampling\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = torch.log(1. - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = torch.sqrt(1. / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1. / self.alphas_cumprod - 1)\n",
    "        \n",
    "        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "        \n",
    "    def q_sample(self, x_0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion process: Add noise to the original data.\n",
    "        \n",
    "        Implements q(x_t | x_0) - adding noise over multiple timesteps.\n",
    "        \n",
    "        Args:\n",
    "            x_0 (tensor): Original clean images [B, C, H, W]\n",
    "            t (tensor): Timesteps [B]\n",
    "            noise (tensor, optional): Noise to add (random if None)\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Noisy images at timestep t\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        \n",
    "        # Extract the appropriate alpha and sigma for the given timesteps\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].reshape(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].reshape(-1, 1, 1, 1)\n",
    "        \n",
    "        # Apply forward diffusion formula: x_t = sqrt(α_t)·x_0 + sqrt(1-α_t)·ε\n",
    "        return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    \n",
    "    def p_losses(self, denoise_model, x_0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Calculate loss for training the denoising model.\n",
    "        \n",
    "        The loss is based on predicting the noise that was added at timestep t.\n",
    "        \n",
    "        Args:\n",
    "            denoise_model (nn.Module): UNet model to predict noise\n",
    "            x_0 (tensor): Original clean images [B, C, H, W]\n",
    "            t (tensor): Timesteps [B]\n",
    "            noise (tensor, optional): Noise to add (random if None)\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Mean squared error between predicted and actual noise\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        \n",
    "        # Create noisy samples at timestep t\n",
    "        x_noisy = self.q_sample(x_0, t, noise=noise)\n",
    "        \n",
    "        # Get model's predicted noise\n",
    "        predicted_noise = denoise_model(x_noisy, t)\n",
    "        \n",
    "        # Loss is MSE between predicted and actual noise\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def ddim_sample(self, model, shape, n_steps=150, eta=0.0):\n",
    "        \"\"\"\n",
    "        DDIM sampling for accelerated image generation.\n",
    "        \n",
    "        DDIM (Denoising Diffusion Implicit Models) allows for much faster\n",
    "        sampling than DDPM while maintaining quality. The eta parameter\n",
    "        controls stochasticity (0 = deterministic, 1 = DDPM-like).\n",
    "        \n",
    "        Args:\n",
    "            model (nn.Module): Trained UNet denoising model\n",
    "            shape (tuple): Shape of samples to generate [B, C, H, W]\n",
    "            n_steps (int): Number of sampling steps (fewer than training timesteps)\n",
    "            eta (float): Controls stochasticity (0 = deterministic, 1 = DDPM-like)\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Generated samples [B, C, H, W]\n",
    "        \"\"\"\n",
    "        device = next(model.parameters()).device\n",
    "        b = shape[0]\n",
    "        \n",
    "        # Start from pure noise\n",
    "        img = torch.randn(shape, device=device)\n",
    "        \n",
    "        # Select subset of timesteps for DDIM sampling (evenly spaced)\n",
    "        timesteps = np.linspace(0, self.timesteps - 1, n_steps, dtype=int)[::-1]\n",
    "        \n",
    "        # Progressively denoise the image\n",
    "        for i in tqdm(range(len(timesteps) - 1), desc='DDIM Sampling'):\n",
    "            t_current = torch.full((b,), timesteps[i], device=device, dtype=torch.long)\n",
    "            t_next = torch.full((b,), timesteps[i + 1], device=device, dtype=torch.long)\n",
    "            \n",
    "            # Predict noise\n",
    "            with torch.no_grad():\n",
    "                predicted_noise = model(img, t_current)\n",
    "            \n",
    "            # Extract x0 from xt using the predicted noise\n",
    "            alpha_cumprod_t = self.alphas_cumprod[t_current].reshape(-1, 1, 1, 1)\n",
    "            alpha_cumprod_next = self.alphas_cumprod[t_next].reshape(-1, 1, 1, 1)\n",
    "            \n",
    "            sqrt_alpha_cumprod_t = torch.sqrt(alpha_cumprod_t)\n",
    "            sqrt_one_minus_alpha_cumprod_t = torch.sqrt(1 - alpha_cumprod_t)\n",
    "            \n",
    "            # Predict x0 from xt and predicted noise\n",
    "            predicted_x0 = (img - sqrt_one_minus_alpha_cumprod_t * predicted_noise) / sqrt_alpha_cumprod_t\n",
    "            \n",
    "            # Calculate coefficient for predicted_x0\n",
    "            sqrt_alpha_cumprod_next = torch.sqrt(alpha_cumprod_next)\n",
    "            \n",
    "            # Calculate coefficient for direction pointing to xt\n",
    "            # Interpolate between DDPM and DDIM using eta\n",
    "            sigma_t = eta * torch.sqrt((1 - alpha_cumprod_next) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_next))\n",
    "            \n",
    "            # Get noise for stochastic part (if eta > 0)\n",
    "            noise = torch.randn_like(img) if eta > 0 else 0\n",
    "            \n",
    "            # Compute the next sample using DDIM formula\n",
    "            img = sqrt_alpha_cumprod_next * predicted_x0 + \\\n",
    "                  torch.sqrt(1 - alpha_cumprod_next - sigma_t**2) * predicted_noise + \\\n",
    "                  sigma_t * noise\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c2c66-1d63-4f54-b312-a7f028f1260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_diffusion_model(dataloader, model, diffusion, optimizer, config):\n",
    "    \"\"\"\n",
    "    Train the diffusion model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: DataLoader containing training data\n",
    "        model: UNet model to train\n",
    "        diffusion: DiffusionModel instance\n",
    "        optimizer: Optimizer instance (e.g., Adam)\n",
    "        config: Dictionary with training configuration\n",
    "        \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    # Move model to appropriate device\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # Training loop over epochs\n",
    "    for epoch in range(config['epochs']):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n",
    "        \n",
    "        for step, batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Move batch to device\n",
    "            batch = batch.to(device)\n",
    "            batch_size = batch.shape[0]\n",
    "            \n",
    "            # Sample random timesteps for each image in batch\n",
    "            t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device).long()\n",
    "            \n",
    "            # Calculate loss (predict noise added at timestep t)\n",
    "            loss = diffusion.p_losses(model, batch, t)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "            \n",
    "        # Print average loss for epoch\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']}, Average Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        # Generate and save samples every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            # Save model checkpoint\n",
    "            save_path = os.path.join(config['save_dir'], f\"model_epoch_{epoch+1}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss\n",
    "            }, save_path)\n",
    "            \n",
    "            # Generate samples\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Generate 4 sample images using DDIM sampling\n",
    "                samples = diffusion.ddim_sample(\n",
    "                    model, \n",
    "                    (4, 1, 256, 256), \n",
    "                    n_steps=config['sample_steps']\n",
    "                )\n",
    "                \n",
    "                # Save samples as image\n",
    "                samples_path = os.path.join(config['save_dir'], f\"samples_epoch_{epoch+1}.png\")\n",
    "                show_microstructure_batch(samples, n=4, title=f\"Epoch {epoch+1} Samples\", save_path=samples_path)\n",
    "                \n",
    "                # Also display the samples\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                for i in range(4):\n",
    "                    plt.subplot(2, 2, i+1)\n",
    "                    plt.imshow(((samples[i] + 1) / 2).squeeze().cpu().numpy(), cmap='gray')\n",
    "                    plt.title(f\"Sample {i+1}\")\n",
    "                    plt.axis('off')\n",
    "                plt.suptitle(f\"Generated Samples at Epoch {epoch+1}\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            # Return to training mode\n",
    "            model.train()\n",
    "    \n",
    "    # Save final model\n",
    "    save_path = os.path.join(config['save_dir'], \"final_model.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, save_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f64b6-4e4e-4ff5-b133-80930ae5d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics - FID/IS\n",
    "class InceptionV3(nn.Module):\n",
    "    \"\"\"\n",
    "    Pretrained InceptionV3 network for FID and IS computation.\n",
    "    \n",
    "    This class extracts features from the InceptionV3 model, split into\n",
    "    specific blocks corresponding to the layers needed for FID and IS metrics.\n",
    "    \n",
    "    FID (Fréchet Inception Distance) and IS (Inception Score) are common metrics\n",
    "    for evaluating the quality of generated images.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(InceptionV3, self).__init__()\n",
    "        # Load pretrained InceptionV3 model\n",
    "        inception = models.inception_v3(pretrained=True)\n",
    "        \n",
    "        # Split the model into blocks for feature extraction\n",
    "        # Block 1: Initial convolutions and pooling\n",
    "        self.block1 = nn.Sequential(\n",
    "            inception.Conv2d_1a_3x3, inception.Conv2d_2a_3x3,\n",
    "            inception.Conv2d_2b_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Block 2: More convolutions and pooling\n",
    "        self.block2 = nn.Sequential(\n",
    "            inception.Conv2d_3b_1x1, inception.Conv2d_4a_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Block 3: Mixed inception modules (5b-6e)\n",
    "        self.block3 = nn.Sequential(\n",
    "            inception.Mixed_5b, inception.Mixed_5c, inception.Mixed_5d,\n",
    "            inception.Mixed_6a, inception.Mixed_6b, inception.Mixed_6c,\n",
    "            inception.Mixed_6d, inception.Mixed_6e\n",
    "        )\n",
    "        \n",
    "        # Block 4: Final inception modules, pooling and flattening\n",
    "        # This is the feature extraction layer used for FID\n",
    "        self.block4 = nn.Sequential(\n",
    "            inception.Mixed_7a, inception.Mixed_7b, inception.Mixed_7c,\n",
    "            adaptive_avg_pool2d, nn.Flatten()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Extract features for FID calculation\n",
    "        \n",
    "        Args:\n",
    "            x (tensor): Input images [B, C, H, W]\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Features for FID/IS computation [B, 2048]\n",
    "        \"\"\"\n",
    "        # Resize input to 299x299 as required by InceptionV3\n",
    "        x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # If grayscale, repeat to 3 channels (InceptionV3 expects RGB)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        # Forward pass through blocks\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2966f3-792e-4a80-9317-d9c0240413a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FID and IS Calculation Functions\n",
    "def calculate_activation_statistics(images, model, batch_size=64, device='cuda'):\n",
    "    \"\"\"\n",
    "    Calculate mean and covariance of features for FID calculation.\n",
    "    \n",
    "    Args:\n",
    "        images (tensor): Batch of images [B, C, H, W]\n",
    "        model (nn.Module): Inception model for feature extraction\n",
    "        batch_size (int): Batch size for feature extraction\n",
    "        device (str): Device to run computation on\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (mean, covariance) of the activation statistics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Process images in batches to avoid memory issues\n",
    "    n_batches = len(images) // batch_size + 1\n",
    "    \n",
    "    # Initialize storage for activations (2048 is InceptionV3 feature dimension)\n",
    "    act = np.empty((len(images), 2048))\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        # Handle last batch that might be smaller\n",
    "        if end > len(images):\n",
    "            end = len(images)\n",
    "        \n",
    "        # Extract features for current batch\n",
    "        batch = images[start:end].to(device)\n",
    "        with torch.no_grad():\n",
    "            act[start:end] = model(batch).cpu().numpy()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mu = np.mean(act, axis=0)  # Mean across samples\n",
    "    sigma = np.cov(act, rowvar=False)  # Covariance matrix\n",
    "    return mu, sigma\n",
    "\n",
    "def calculate_fid(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate Fréchet Inception Distance between two sets of images.\n",
    "    \n",
    "    FID measures the distance between two distributions in feature space.\n",
    "    Lower values indicate more similar distributions (better generation quality).\n",
    "    \n",
    "    Args:\n",
    "        mu1, mu2 (ndarray): Mean feature vectors for real and generated images\n",
    "        sigma1, sigma2 (ndarray): Covariance matrices for real and generated images\n",
    "        eps (float): Small value to avoid numerical issues\n",
    "        \n",
    "    Returns:\n",
    "        float: FID score\n",
    "    \"\"\"\n",
    "    # Calculate squared distance between means\n",
    "    diff = mu1 - mu2\n",
    "    \n",
    "    # Calculate sqrt of product of covariances (may be numerically unstable)\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    \n",
    "    # Ensure covmean is well-behaved (not containing NaN or Inf)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        # Add small offset to diagonal for numerical stability\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "    \n",
    "    # Handle complex values from sqrt of matrices\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    # Calculate trace of covmean\n",
    "    tr_covmean = np.trace(covmean)\n",
    "    \n",
    "    # FID formula: ||μ_1 - μ_2||^2 + Tr(Σ_1 + Σ_2 - 2√(Σ_1Σ_2))\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "\n",
    "def calculate_inception_score(samples, model, batch_size=64, splits=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Calculate Inception Score for a batch of generated images.\n",
    "    \n",
    "    IS measures both quality and diversity of generated images.\n",
    "    Higher values indicate better generation quality.\n",
    "    \n",
    "    Args:\n",
    "        samples (tensor): Batch of generated images [B, C, H, W]\n",
    "        model (nn.Module): Inception model for class probabilities\n",
    "        batch_size (int): Batch size for processing\n",
    "        splits (int): Number of splits for calculating statistics\n",
    "        device (str): Device to run computation on\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (mean, std) of the inception score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    N = len(samples)\n",
    "    \n",
    "    # Storage for class probabilities (1000 ImageNet classes)\n",
    "    preds = np.zeros((N, 1000))\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, N, batch_size):\n",
    "        # Extract current batch\n",
    "        batch = samples[i:i+batch_size].to(device)\n",
    "        \n",
    "        # Convert grayscale to RGB if needed\n",
    "        if batch.shape[1] == 1:\n",
    "            batch = batch.repeat(1, 3, 1, 1)\n",
    "            \n",
    "        # Resize to InceptionV3 input size\n",
    "        batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Get softmax predictions (class probabilities)\n",
    "        with torch.no_grad():\n",
    "            pred = F.softmax(model(batch), dim=1).cpu().numpy()\n",
    "            \n",
    "        preds[i:i+batch.shape[0]] = pred\n",
    "    \n",
    "    # Split predictions for calculation\n",
    "    scores = []\n",
    "    for i in range(splits):\n",
    "        # Take subset of predictions for current split\n",
    "        part = preds[i * (N // splits):(i + 1) * (N // splits), :]\n",
    "        \n",
    "        # Calculate KL divergence between probabilities and their mean\n",
    "        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
    "        kl = np.mean(np.sum(kl, 1))\n",
    "        \n",
    "        # Inception score is exp of KL divergence\n",
    "        scores.append(np.exp(kl))\n",
    "    \n",
    "    # Return mean and standard deviation across splits\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "def compute_fid_and_is(real_images, generated_images, device='cuda'):\n",
    "    \"\"\"\n",
    "    Compute FID and IS metrics for real and generated images.\n",
    "    \n",
    "    Args:\n",
    "        real_images (tensor): Batch of real images [B, C, H, W] in range [-1, 1]\n",
    "        generated_images (tensor): Batch of generated images [B, C, H, W] in range [-1, 1]\n",
    "        device (str): Device to run models on\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with FID and IS scores\n",
    "    \"\"\"\n",
    "    # Normalize to [0, 1] range if needed\n",
    "    if real_images.min() < 0:\n",
    "        real_images = (real_images + 1) / 2\n",
    "    if generated_images.min() < 0:\n",
    "        generated_images = (generated_images + 1) / 2\n",
    "    \n",
    "    # Load Inception model\n",
    "    inception_model = InceptionV3().to(device)\n",
    "    inception_model.eval()\n",
    "    \n",
    "    # Calculate FID\n",
    "    mu_real, sigma_real = calculate_activation_statistics(real_images, inception_model, device=device)\n",
    "    mu_gen, sigma_gen = calculate_activation_statistics(generated_images, inception_model, device=device)\n",
    "    fid_value = calculate_fid(mu_real, sigma_real, mu_gen, sigma_gen)\n",
    "    \n",
    "    # Calculate IS for generated images\n",
    "    is_mean, is_std = calculate_inception_score(generated_images, inception_model, device=device)\n",
    "    \n",
    "    return {\n",
    "        'fid': fid_value,\n",
    "        'is_mean': is_mean,\n",
    "        'is_std': is_std\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97911ba6-02e7-4915-9619-1531c3d454eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Material-Specific Evaluation Metrics\n",
    "def two_point_correlation(image, max_distance=None, debug=False):\n",
    "    \"\"\"\n",
    "    Compute two-point correlation function for a binary image with robust error handling.\n",
    "    \n",
    "    The two-point correlation function S2(r) measures the probability that two points\n",
    "    separated by distance r both lie in the same phase of the material. It's a key\n",
    "    statistical descriptor used in materials science to characterize microstructures.\n",
    "    \n",
    "    Args:\n",
    "        image (tensor or ndarray): Input image to analyze\n",
    "        max_distance (int, optional): Maximum distance to compute correlation\n",
    "        debug (bool): Whether to print debugging information\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (distances, correlation values)\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy if needed\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.cpu().numpy()\n",
    "    \n",
    "    image = image.squeeze()\n",
    "    \n",
    "    # Handle edge case: nearly constant images\n",
    "    if np.all(image < 0.05) or np.all(image > 0.95):\n",
    "        if debug:\n",
    "            print(f\"Warning: Image is nearly constant (min={image.min()}, max={image.max()})\")\n",
    "        # Return a flat correlation function\n",
    "        if max_distance is None:\n",
    "            max_distance = min(image.shape) // 2\n",
    "        return np.arange(max_distance), np.zeros(max_distance)\n",
    "    \n",
    "    # Normalize to [0,1] range if needed\n",
    "    if image.max() > 1:\n",
    "        image = image / 255.0\n",
    "    \n",
    "    # Binarize with a threshold (0.5) for two-phase microstructure\n",
    "    binary_img = (image > 0.5).astype(np.float32)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Binary image stats: min={binary_img.min()}, max={binary_img.max()}, mean={binary_img.mean()}\")\n",
    "    \n",
    "    h, w = binary_img.shape\n",
    "    if max_distance is None:\n",
    "        max_distance = min(h, w) // 2\n",
    "    \n",
    "    try:\n",
    "        # Calculate autocorrelation using Fast Fourier Transform (FFT) for efficiency\n",
    "        # This is much faster than direct calculation for large images\n",
    "        ft = np.fft.fft2(binary_img)\n",
    "        power_spectrum = np.abs(np.fft.ifft2(ft * np.conj(ft)))**2\n",
    "        center = np.fft.fftshift(power_spectrum)[h//2, w//2]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"FFT stats: power_spectrum mean={np.mean(power_spectrum)}, center={center}\")\n",
    "        \n",
    "        # Compute radial average (azimuthal integration)\n",
    "        y, x = np.indices((h, w))\n",
    "        r = np.sqrt((x - w//2)**2 + (y - h//2)**2).astype(np.int32)\n",
    "        r_max = min(h//2, w//2, max_distance)\n",
    "        \n",
    "        # Handle edge case: very small images\n",
    "        if r_max < 2:\n",
    "            if debug:\n",
    "                print(f\"Warning: r_max is too small ({r_max})\")\n",
    "            r_max = 2\n",
    "        \n",
    "        # Calculate binned average safely\n",
    "        tbin = np.bincount(r.ravel(), power_spectrum.ravel())\n",
    "        nr = np.bincount(r.ravel())\n",
    "        \n",
    "        # Check for division by zero\n",
    "        valid_indices = np.where(nr[:r_max] > 0)[0]\n",
    "        \n",
    "        if len(valid_indices) == 0:\n",
    "            if debug:\n",
    "                print(\"Warning: No valid indices for radial profile\")\n",
    "            return np.arange(r_max), np.zeros(r_max)\n",
    "        \n",
    "        # Initialize array and populate only valid indices\n",
    "        radial_profile = np.zeros(r_max)\n",
    "        radial_profile[valid_indices] = tbin[valid_indices] / nr[valid_indices]\n",
    "        \n",
    "        # Normalize only if center is not zero\n",
    "        if abs(center) > 1e-10:\n",
    "            radial_profile = radial_profile / center\n",
    "        \n",
    "        # Replace any NaN or inf values with zeros for stability\n",
    "        radial_profile = np.nan_to_num(radial_profile, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Radial profile stats: min={radial_profile.min()}, max={radial_profile.max()}, mean={radial_profile.mean()}\")\n",
    "        \n",
    "        return np.arange(r_max), radial_profile\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Robust error handling - return zeros instead of crashing\n",
    "        if debug:\n",
    "            print(f\"Error in two_point_correlation: {str(e)}\")\n",
    "        return np.arange(max_distance or 10), np.zeros(max_distance or 10)\n",
    "\n",
    "def lineal_path_function(image, max_distance=None):\n",
    "    \"\"\"\n",
    "    Compute lineal path function for a binary image.\n",
    "    \n",
    "    The lineal path function L(r) gives the probability that a line segment of\n",
    "    length r lies completely in one phase of the material. It characterizes the\n",
    "    connectivity of the phases in the microstructure.\n",
    "    \n",
    "    Args:\n",
    "        image (tensor or ndarray): Input image to analyze\n",
    "        max_distance (int, optional): Maximum distance to compute\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (distances, lineal path values)\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy if needed\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.cpu().numpy()\n",
    "    \n",
    "    image = image.squeeze()\n",
    "    if image.max() > 1:\n",
    "        image = image / 255.0\n",
    "    \n",
    "    # Binarize the image\n",
    "    image = (image > 0.5).astype(np.float32)\n",
    "    \n",
    "    h, w = image.shape\n",
    "    if max_distance is None:\n",
    "        # Limit to 1/4 of the smaller dimension for computational efficiency\n",
    "        max_distance = min(h, w) // 4\n",
    "    \n",
    "    # Initialize storage for lineal path function\n",
    "    lp = np.zeros(max_distance)\n",
    "    counts = np.zeros(max_distance)\n",
    "    \n",
    "    # Sample a subset of points for efficiency (Monte Carlo approach)\n",
    "    n_samples = 1000\n",
    "    y_samples = np.random.randint(0, h, n_samples)\n",
    "    x_samples = np.random.randint(0, w, n_samples)\n",
    "    \n",
    "    # Check horizontal lines\n",
    "    for i, x in enumerate(x_samples):\n",
    "        y = y_samples[i]\n",
    "        if x + max_distance <= w:\n",
    "            line = image[y, x:x+max_distance]\n",
    "            for l in range(1, max_distance):\n",
    "                # Check if all pixels in the line segment are in the phase (value > 0.5)\n",
    "                if np.all(line[:l+1] > 0.5):\n",
    "                    lp[l] += 1\n",
    "                counts[l] += 1\n",
    "    \n",
    "    # Check vertical lines\n",
    "    for i, y in enumerate(y_samples):\n",
    "        x = x_samples[i]\n",
    "        if y + max_distance <= h:\n",
    "            line = image[y:y+max_distance, x]\n",
    "            for l in range(1, max_distance):\n",
    "                if np.all(line[:l+1] > 0.5):\n",
    "                    lp[l] += 1\n",
    "                counts[l] += 1\n",
    "    \n",
    "    # Normalize safely (avoid division by zero)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        lp = np.divide(lp, counts, out=np.zeros_like(lp), where=counts!=0)\n",
    "    \n",
    "    return np.arange(max_distance), (lp_orig - lp_gen, r_orig)\n",
    "        if area_orig > 0:\n",
    "            lineal_discrepancy = (area_between / area_orig) * 100\n",
    "        else:\n",
    "            lineal_discrepancy = 0\n",
    "        metrics['lineal_discrepancy'].append(lineal_discrepancy)\n",
    "    \n",
    "    # Compute average metrics\n",
    "    metrics['avg_ssim'] = np.mean(metrics['ssim_values'])\n",
    "    metrics['avg_s2_discrepancy'] = np.mean(metrics['s2_discrepancy'])\n",
    "    metrics['avg_lineal_discrepancy'] = np.mean(metrics['lineal_discrepancy'])\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5fb9b-e3b2-44f1-a1f0-c2e406de99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution function\n",
    "def run_training(config):\n",
    "    \"\"\"Set up and run the training process.\"\"\"\n",
    "    print(\"Configuration:\", config)\n",
    "    \n",
    "    # Create save directory\n",
    "    os.makedirs(config['save_dir'], exist_ok=True)\n",
    "    \n",
    "    # Save configuration\n",
    "    with open(os.path.join(config['save_dir'], 'config.txt'), 'w') as f:\n",
    "        for key, value in config.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomAffine(degrees=10, scale=(0.9, 1.1)),\n",
    "    ])\n",
    "    \n",
    "    # Load the dataset\n",
    "    dataset = Micro2DKeyDataset(\n",
    "        file_path=config['file_path'], \n",
    "        key=config['microstructure_class'],\n",
    "        transform=transform if config['use_augmentation'] else None\n",
    "    )\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    \n",
    "    # Show sample images\n",
    "    sample_batch = next(iter(data_loader))\n",
    "    show_microstructure_batch(\n",
    "        sample_batch, \n",
    "        n=4, \n",
    "        title=f\"{config['microstructure_class']} Samples\",\n",
    "        save_path=os.path.join(config['save_dir'], 'original_samples.png')\n",
    "    )\n",
    "    \n",
    "    # Initialize model and diffusion\n",
    "    model = UNet(in_channels=1, out_channels=1)\n",
    "    diffusion = DiffusionModel(\n",
    "        timesteps=config['timesteps'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = train_diffusion_model(\n",
    "        dataloader=data_loader,\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        optimizer=optimizer,\n",
    "        config=config\n",
    "    )\n",
    "    # Evaluate and visulaize the results \n",
    "    metrics = evaluate_and_visualize(\n",
    "        original_dataset=dataset,\n",
    "        model=trained_model,\n",
    "        diffusion=diffusion,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Model and samples saved in: {config['save_dir']}\")\n",
    "    print(f\"Average SSIM: {metrics['avg_ssim']:.4f}\")\n",
    "    print(f\"Average Two-Point Correlation Discrepancy: {metrics['avg_s2_discrepancy']:.2f}%\")\n",
    "    print(f\"Average Lineal Path Discrepancy: {metrics['avg_lineal_discrepancy']:.2f}%\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'file_path': '/lustre/uschill-lab/users/3782/diffusion/New_Diff/MICRO2D_homogenized.h5',  # Path to HDF5 file\n",
    "        'microstructure_class': 'VoidSmall',  # Class \n",
    "        'batch_size':16 ,\n",
    "        'epochs': 50,\n",
    "        'learning_rate': 5e-5,\n",
    "        'weight_decay': 1e-5,\n",
    "        'timesteps': 2000,  # Number of diffusion steps\n",
    "        'sample_steps': 150,  # Steps for DDIM sampling\n",
    "        'use_augmentation': True,\n",
    "        'num_workers': 2,\n",
    "        'save_dir': f'/lustre/uschill-lab/users/3782/diffusion/New_Diff/diffusion_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    }\n",
    "    \n",
    "    run_training(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
