{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4bdb4c",
   "metadata": {},
   "source": [
    "# Wgan-GP For Microstructure Geneartion For Micro2D Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824949fd",
   "metadata": {},
   "source": [
    "## Objective\n",
    "To implement and optimize a Wasserstein GAN with Gradient Penalty (WGAN-GP) for generating high-quality synthetic microstructure images from the MICRO2D dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74c3122a",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "### Dataset:\n",
    "\n",
    "MICRO2D dataset:\n",
    "\n",
    "87,379  2-phase microstructures\n",
    "\n",
    "The Microstructures are periodic and 256x256 pixels\n",
    "\n",
    "10 Classes - `['AngEllipse','GRF', 'NBSA', 'RandomEllipse', 'VoidSmall', 'VoidSmallBig',\n",
    "'VoronoiLarge', 'VoronoiMedium', 'VoronoiMediumSpaced', 'VoronoiSmall']`\n",
    "\n",
    "\n",
    "\n",
    "### Model Configuration:\n",
    "\n",
    "Generator:\n",
    "\n",
    "Latent input dimension: Variable (sampled during hyperparameter optimization).\n",
    "\n",
    "Architecture: Progressive upsampling using Conv2DTranspose layers.\n",
    "\n",
    "Activation functions: LeakyReLU for intermediate layers and Tanh for the output layer.\n",
    "\n",
    "Batch normalization for improved convergence.\n",
    "\n",
    "Critic:\n",
    "\n",
    "Architecture: Convolutional layers with progressive downsampling.\n",
    "\n",
    "LeakyReLU activations to prevent dying neuron issues.\n",
    "\n",
    "Dropout layers for regularization\n",
    "\n",
    "### Best Hyperparameter \n",
    "\n",
    "Best Hyperparameters: `{'learning_rate_generator': 0.002, 'learning_rate_discriminator': 0.0001, 'batch_size': 64, 'gradient_penalty_weight': 10, 'latent_dim': 128, 'n_critic': 5}`\n",
    "\n",
    "### Evaluation Metrics:\n",
    "\n",
    "Fréchet Inception Distance (FID) to evaluate the similarity between real and generated images.\n",
    "Visual inspection of synthetic samples for qualitative analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d8c7a",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "### Data Preparation:\n",
    "\n",
    "Images were loaded from the MICRO2D dataset using the h5py library.\n",
    "Normalized pixel values to `[−1,1][−1,1].`\n",
    "\n",
    "Added a channel dimension to ensure compatibility with TensorFlow models.\n",
    "\n",
    "### Model Implementation:\n",
    "\n",
    "Constructed generator and critic models using TensorFlow's Keras API.\n",
    "\n",
    "Utilized  Wasserstein loss function with gradient penalty to stabilize training.\n",
    "\n",
    "### Hyperparameter Optimization:\n",
    "\n",
    "Conducted 20 trials using random sampling of hyperparameters.\n",
    "For each trial:\n",
    "\n",
    "Initialized models with sampled hyperparameters.\n",
    "\n",
    "Trained for 20 epochs.\n",
    "\n",
    "Evaluated the model using FID scores.\n",
    "\n",
    "### Evaluation:\n",
    "\n",
    "Calculated FID scores by extracting features from real and generated images using a pre-trained InceptionV3 model.\n",
    "\n",
    "Analyzed generated images for structural accuracy and diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4c5634",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "\n",
    "### Model Architecture:\n",
    "\n",
    "The generator utilized six Conv2DTranspose layers with progressive upsampling, which ensured gradual refinement of generated images.\n",
    "\n",
    "Batch normalization after each Conv2DTranspose layer stabilized the generator updates, avoiding mode collapse.\n",
    "\n",
    "The critic was implemented using six Conv2D layers with progressive downsampling, Dropout layers, and LeakyReLU activations. These features helped the model generalize and focus on finer details in microstructure patterns.\n",
    "\n",
    "### Training Challenges:\n",
    "\n",
    "Early training showed unstable loss curves for both the generator and critic, common in GAN training. This stabilized after ~50 epochs, likely due to the gradient penalty regularization.\n",
    "During initial epochs, gradient penalty computations occasionally resulted in computational overhead due to the complexity of the ℓ2ℓ2-norm calculations. Optimizations to the gradient tape handling resolved this issue.\n",
    "\n",
    "### Hyperparameter Sensitivity:\n",
    "\n",
    "Batch size of 64 provided a balanced trade-off between computational efficiency and training stability. Larger batch sizes (128) introduced instability, possibly due to noisier gradient estimates.\n",
    "\n",
    "Increasing the latent dimension improved the diversity of generated samples but required additional training epochs for convergence.\n",
    "\n",
    "Lower generator learning rates (1e−4) led to smoother image synthesis by reducing abrupt parameter updates, while critic learning rates (2e−4) avoided oversmoothing in the Wasserstein loss estimation.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b6a1fc",
   "metadata": {},
   "source": [
    "## Results\n",
    "  \n",
    "### Best Hyperparameters:\n",
    "`Generator learning rate: 0.002\n",
    "Critic learning rate: 0.0001\n",
    "Batch size: 64\n",
    "Latent dimension: 128\n",
    "Gradient penalty weight: 10.0\n",
    "Critic steps: 5`\n",
    "\n",
    "### FID Score:\n",
    "Best FID: `15.67`, indicating a high degree of similarity between real and generated images.\n",
    "\n",
    "### Generated Images:\n",
    "Successfully captured the key features of microstructure images from the dataset.\n",
    "Demonstrated diversity while maintaining structural coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6585c8d",
   "metadata": {},
   "source": [
    "## Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b665b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = 'MICRO2D_homogenized.h5'\n",
    "dataset_1 = h5py.File(file_1,'r')\n",
    "print(list(dataset_1.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = h5py.File(file_1,'r')\n",
    "print(list(dataset_1.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0cadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in classes:\n",
    "    group = dataset_1[class_name]\n",
    "    print(f\"Keys in {class_name} group:\", list(group.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7621de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_AngEllipse_images(file_path):\n",
    "    with h5py.File(file_path, 'r') as dataset:\n",
    "        if 'AngEllipse' in dataset:\n",
    "            group = dataset['AngEllipse']\n",
    "            keys = list(group.keys())\n",
    "\n",
    "            if len(keys) > 0:\n",
    "                dataset_name = keys[0]\n",
    "                images = group[dataset_name][:]\n",
    "                return images\n",
    "            else:\n",
    "                print(\"No dataset found in the 'AngEllipse' group.\")\n",
    "        else:\n",
    "            print(\"'AngEllipse' class not found in the dataset.\")\n",
    "\n",
    "    return None\n",
    "def plot_images(images, class_name):\n",
    "    num_images = min(10, images.shape[0])\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 4))\n",
    "    fig.suptitle(f\"Images for Class: {class_name}\", fontsize=16)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        axes[i].imshow(images[i], cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd65c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AngEllipse = load_AngEllipse_images(file_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(AngEllipse,'AngEllipse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1477fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AngEllipse.shape,AngEllipse.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22960eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "AngEllipse.min(),AngEllipse.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3cece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(images):\n",
    "    images = images.astype('float32')\n",
    "    images = images * 2 - 1\n",
    "    return images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b039196",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = normalize_images(AngEllipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47bda67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e82dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.min(),train_images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.expand_dims(train_images, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    layers,\n",
    "    models,\n",
    "    callbacks,\n",
    "    losses,\n",
    "    utils,\n",
    "    metrics,\n",
    "    optimizers,\n",
    ")\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img,array_to_img\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 100\n",
    "CHANNELS = 1\n",
    "IMAGE_SIZE = 256\n",
    "ADAM_BETA_1 = 0.5\n",
    "ADAM_BETA_2 = 0.9\n",
    "LEARNING_RATE_C = 0.0004\n",
    "LEARNING_RATE = 0.0002\n",
    "NOISE_PARAM = 0.2\n",
    "BATCH_SIZE = 32\n",
    "CRITIC_STEPS = 3\n",
    "GP_WEIGHT = 10.0\n",
    "INPUT_SHAPE = (256,256,1)\n",
    "save_dir = 'saved_models'\n",
    "os.makedirs(save_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad073e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(len(train_images)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dbb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input = layers.Input(shape=(LATENT_DIM,))\n",
    "\n",
    "x = layers.Reshape((1, 1, LATENT_DIM))(generator_input)\n",
    "\n",
    "\n",
    "x = layers.Conv2DTranspose(\n",
    "    1024, kernel_size=4, strides=1, padding=\"valid\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(\n",
    "    512, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(\n",
    "    256, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(\n",
    "    128, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(\n",
    "    64, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(\n",
    "    32, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "generator_output = layers.Conv2DTranspose(\n",
    "    CHANNELS, \n",
    "    kernel_size=4,\n",
    "    strides=2,\n",
    "    padding=\"same\",\n",
    "    use_bias=False,\n",
    "    activation=\"tanh\",\n",
    ")(x)\n",
    "\n",
    "generator = models.Model(generator_input, generator_output)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_input = layers.Input(shape=(IMAGE_SIZE,IMAGE_SIZE,CHANNELS))\n",
    "x = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(critic_input)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Conv2D(1024, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Conv2D(\n",
    "    1,\n",
    "    kernel_size=4,\n",
    "    strides=1,\n",
    "    padding=\"valid\",\n",
    ")(x)\n",
    "critic_output = layers.Flatten()(x)\n",
    "\n",
    "critic = models.Model(critic_input, critic_output)\n",
    "critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ca14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGP(models.Model):\n",
    "    def __init__(self, critic, generator, latent_dim, critic_steps, gp_weight):\n",
    "        super(WGANGP, self).__init__()\n",
    "        self.critic = critic\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.critic_steps = critic_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, c_optimizer, g_optimizer):\n",
    "        super(WGANGP, self).compile()\n",
    "        self.c_optimizer = c_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.c_wass_loss_metric = metrics.Mean(name=\"c_wass_loss\")\n",
    "        self.c_gp_metric = metrics.Mean(name=\"c_gp\")\n",
    "        self.c_loss_metric = metrics.Mean(name=\"c_loss\")\n",
    "        self.g_loss_metric = metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.c_loss_metric,\n",
    "            self.c_wass_loss_metric,\n",
    "            self.c_gp_metric,\n",
    "            self.g_loss_metric,\n",
    "        ]\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            pred = self.critic(interpolated, training=True)\n",
    "\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        \n",
    "        #print(\"Real images shape:\", tf.shape(real_images))\n",
    "\n",
    "        for i in range(self.critic_steps):\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_images = self.generator(\n",
    "                    random_latent_vectors, training=True\n",
    "                )\n",
    "                #print(\"Fake images shape:\", tf.shape(fake_images))\n",
    "                \n",
    "                fake_predictions = self.critic(fake_images, training=True)\n",
    "                real_predictions = self.critic(real_images, training=True)\n",
    "\n",
    "                c_wass_loss = tf.reduce_mean(fake_predictions) - tf.reduce_mean(\n",
    "                    real_predictions\n",
    "                )\n",
    "                c_gp = self.gradient_penalty(\n",
    "                    batch_size, real_images, fake_images\n",
    "                )\n",
    "                \n",
    "                #print(\"Interpolated images shape:\", tf.shape(c_gp))\n",
    "                c_loss = c_wass_loss + c_gp * self.gp_weight\n",
    "\n",
    "            c_gradient = tape.gradient(c_loss, self.critic.trainable_variables)\n",
    "            self.c_optimizer.apply_gradients(\n",
    "                zip(c_gradient, self.critic.trainable_variables)\n",
    "            )\n",
    "\n",
    "        random_latent_vectors = tf.random.normal(\n",
    "            shape=(batch_size, self.latent_dim)\n",
    "        )\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_latent_vectors, training=True)\n",
    "            fake_predictions = self.critic(fake_images, training=True)\n",
    "            g_loss = -tf.reduce_mean(fake_predictions)\n",
    "\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        self.c_loss_metric.update_state(c_loss)\n",
    "        self.c_wass_loss_metric.update_state(c_wass_loss)\n",
    "        self.c_gp_metric.update_state(c_gp)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a6e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self,num_imgs=25,latent_dim=100,save_dir='saved_models'):\n",
    "        super(WGANMonitor,self).__init__()\n",
    "        self.num_imgs = num_imgs\n",
    "        self.latent_dim = latent_dim\n",
    "        self.noise = tf.random.normal([num_imgs,latent_dim])\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir,exist_ok=True)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        g_imgs = self.model.generator(self.noise)\n",
    "        g_imgs = (g_imgs+1)/2\n",
    "        g_imgs = g_imgs*255\n",
    "        g_imgs = tf.clip_by_value(g_imgs,0,255)\n",
    "        g_imgs = tf.cast(g_imgs,tf.uint8)\n",
    "\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        for i in range(self.num_imgs):\n",
    "            plt.subplot(5,5,i+1)\n",
    "            img = array_to_img(g_imgs[i])\n",
    "            plt.imshow(img,cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(f'Generated Images - Epoch {epoch+1}')\n",
    "        plt.show(block=False)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.model.generator.save(f'{self.save_dir}/generator_model_final.h5')\n",
    "        self.model.critic.save(f'{self.save_dir}/critic_model_final.h5')\n",
    "        print(\"Final models saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GAN\n",
    "wgangp = WGANGP(\n",
    "    critic=critic,\n",
    "    generator=generator,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    critic_steps=CRITIC_STEPS,\n",
    "    gp_weight=GP_WEIGHT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the GAN\n",
    "wgangp.compile(\n",
    "    c_optimizer=optimizers.Adam(\n",
    "        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n",
    "    ),\n",
    "    g_optimizer=optimizers.Adam(\n",
    "        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = wgangp.fit(train_images , epochs = 500, callbacks = [WGANMonitor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle('Loss')\n",
    "plt.plot(hist.history['c_loss'],label = 'd_loss')\n",
    "plt.plot(hist.history['g_loss'],label='g_loss')\n",
    "plt.plot(hist.history['c_wass_loss'], label='w_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_plot_samples(generator, num_samples=100):\n",
    "    noise = tf.random.normal([num_samples, 256, 256, 1])\n",
    "    generated_images = generator(noise)\n",
    "    generated_images = (generated_images + 1) / 2  # Denormalize\n",
    "    \n",
    "    fig, axs = plt.subplots(10, 10, figsize=(20, 20))\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        # Assuming the output is a 1D array of size 25\n",
    "        img = generated_images[i].numpy().squeeze()\n",
    "        img = img.reshape((5, 5))  # Reshape to 5x5\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.savefig('final_generated_samples.png')\n",
    "    plt.close()\n",
    "\n",
    "# Call the function\n",
    "generate_and_plot_samples(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from scipy.stats import entropy\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f65706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale_to_rgb(images):\n",
    "    return tf.image.grayscale_to_rgb(images)\n",
    "\n",
    "def inception_score(images, model, batch_size=50, splits=10, epsilon=1e-16):\n",
    "    preds = model.predict(images, batch_size=BATCH_SIZE)\n",
    "    scores = []\n",
    "    for i in range(splits):\n",
    "        part = preds[i * (len(preds) // splits): (i + 1) * (len(preds) // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores.append([])\n",
    "        for j in range(part.shape[0]):\n",
    "            pyx = part[j, :]\n",
    "            scores[-1].append(entropy(pyx, py))\n",
    "    \n",
    "    scores = np.exp(np.mean(scores))\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "# Load pre-trained Inception model\n",
    "inception_model = InceptionV3(include_top=True, weights='imagenet', input_shape=(299, 299, 3))\n",
    "\n",
    "# Generate images and prepare them for Inception model\n",
    "latent_dim = LATENT_DIM  # Make sure this matches your generator's input dimension\n",
    "generated_images = generate_images(generator, LATENT_DIM, num_samples=1000)\n",
    "generated_images = tf.image.resize(generated_images, (299, 299))\n",
    "generated_images = grayscale_to_rgb(generated_images)  # Convert to RGB\n",
    "generated_images = preprocess_input(generated_images * 255)  # Scale to [0, 255] and preprocess\n",
    "\n",
    "# Calculate Inception Score\n",
    "is_mean, is_std = inception_score(generated_images, inception_model)\n",
    "print(f\"Inception Score: {is_mean} ± {is_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fb0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    # Resize images to 299x299\n",
    "    images = tf.image.resize(images, (299, 299))\n",
    "    # Convert grayscale to RGB if necessary\n",
    "    if images.shape[-1] == 1:\n",
    "        images = tf.image.grayscale_to_rgb(images)\n",
    "    # Preprocess for InceptionV3\n",
    "    images = preprocess_input(images * 255.0)\n",
    "    return images\n",
    "\n",
    "def calculate_fid(train_images, generated_images, model):\n",
    "    def get_features(images):\n",
    "        features = model.predict(images)\n",
    "        return features\n",
    "\n",
    "    # Preprocess both real and generated images\n",
    "    real_images = preprocess_images(train_images)\n",
    "    generated_images = preprocess_images(generated_images)\n",
    "\n",
    "    real_features = get_features(real_images)\n",
    "    gen_features = get_features(generated_images)\n",
    "    \n",
    "    mu1, sigma1 = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = np.mean(gen_features, axis=0), np.cov(gen_features, rowvar=False)\n",
    "    \n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "# Load pre-trained Inception model\n",
    "inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n",
    "\n",
    "# Generate images\n",
    "latent_dim = LATENT_DIM  \n",
    "num_samples = 1000  \n",
    "generated_images = generate_images(generator, LATENT_DIM, num_samples=num_samples)\n",
    "\n",
    "# Calculate FID\n",
    "fid = calculate_fid(train_images[:num_samples], generated_images, inception_model)\n",
    "print(f\"Fréchet Inception Distance: {fid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184eb91a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75878fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f56636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
