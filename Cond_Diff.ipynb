{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "669688fa-5fa8-4e67-9718-2ec1343b1aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/3782/.conda/envs/diffusion/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from diffusers import UNet2DModel, DDIMScheduler\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b067b713-5fc7-461d-a619-fd5fc4167da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Create output directories\n",
    "os.makedirs(\"../Conditional_Diff/generated_samples\", exist_ok=True)\n",
    "os.makedirs(\"../Conditional_Diff/models\", exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a2c967-0632-4e71-8ee2-47a69dc88e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved dataset class\n",
    "class MICRO2D_Dataset(Dataset):\n",
    "    def __init__(self, file_path, microstructure_class='NBSA', property_type='mechanical', augment=True):\n",
    "        self.file = h5py.File(file_path, 'r')\n",
    "        self.images = self.file[microstructure_class][microstructure_class][:]\n",
    "        \n",
    "        if property_type == 'mechanical':\n",
    "            self.properties = self.file[microstructure_class]['homogenized_mechanical'][:]\n",
    "            self.prop_dim = 1\n",
    "        else:\n",
    "            self.properties = self.file[microstructure_class]['homogenized_thermal'][:]\n",
    "            self.prop_dim = 1\n",
    "            \n",
    "        self.property_type = property_type\n",
    "        self.microstructure_class = microstructure_class\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Precompute property min/max for normalization\n",
    "        all_props = [self.properties[i][0][0] for i in range(len(self))]\n",
    "        self.prop_min = min(all_props)\n",
    "        self.prop_max = max(all_props)\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} {microstructure_class} microstructures\")\n",
    "        print(f\"Property shape: {self.properties.shape}\")\n",
    "        print(f\"Property range: {self.prop_min:.2f} to {self.prop_max:.2f}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get and preprocess image\n",
    "        image = torch.from_numpy(self.images[idx]).float().unsqueeze(0) * 2 - 1\n",
    "        \n",
    "        # Data augmentation\n",
    "        if self.augment:\n",
    "            k = np.random.randint(0, 4)  # 0: no rotation, 1: 90°, 2: 180°, 3: 270°\n",
    "            image = torch.rot90(image, k, dims=[1, 2])\n",
    "            \n",
    "            # Random horizontal and vertical flips\n",
    "            if np.random.random() > 0.5:\n",
    "                image = torch.flip(image, dims=[2])\n",
    "            if np.random.random() > 0.5:\n",
    "                image = torch.flip(image, dims=[1])\n",
    "        \n",
    "        # Get property value\n",
    "        prop_value = torch.tensor([self.properties[idx][0][0]], dtype=torch.float32)\n",
    "        \n",
    "        # Normalize property to [-1, 1] range\n",
    "        norm_prop = 2 * (prop_value - self.prop_min) / (self.prop_max - self.prop_min) - 1\n",
    "        \n",
    "        # Create property channel\n",
    "        prop_channel = norm_prop.view(1, 1, 1).repeat(1, 256, 256)\n",
    "        \n",
    "        # Combine image with property channel\n",
    "        combined_input = torch.cat([image, prop_channel], dim=0)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"combined_input\": combined_input,\n",
    "            \"properties\": prop_value,\n",
    "            \"normalized_prop\": norm_prop,\n",
    "            \"original_idx\": idx\n",
    "        }\n",
    "    \n",
    "    def denormalize_image(self, image):\n",
    "        \"\"\"Convert from [-1, 1] to [0, 1] range\"\"\"\n",
    "        return (image + 1) / 2\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the HDF5 file\"\"\"\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e87c76-b197-4a37-9542-37bcfb47b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze property distribution\n",
    "def analyze_property_distribution(file_path, class_name='NBSA', property_type='mechanical'):\n",
    "    \"\"\"Analyze the distribution of properties in the dataset\"\"\"\n",
    "    \n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        # Get properties\n",
    "        if property_type == 'mechanical':\n",
    "            properties = file[class_name]['homogenized_mechanical'][:]\n",
    "        else:\n",
    "            properties = file[class_name]['homogenized_thermal'][:]\n",
    "        \n",
    "        # Extract the first property of the first material combination\n",
    "        primary_props = [properties[i][0][0] for i in range(len(properties))]\n",
    "        \n",
    "        # Get statistics\n",
    "        min_val = min(primary_props)\n",
    "        max_val = max(primary_props)\n",
    "        mean_val = sum(primary_props) / len(primary_props)\n",
    "        median_val = sorted(primary_props)[len(primary_props)//2]\n",
    "        \n",
    "        print(f\"Property statistics for {class_name} ({property_type}):\")\n",
    "        print(f\"  Min: {min_val:.2f}\")\n",
    "        print(f\"  Max: {max_val:.2f}\")\n",
    "        print(f\"  Mean: {mean_val:.2f}\")\n",
    "        print(f\"  Median: {median_val:.2f}\")\n",
    "        \n",
    "        # Create histogram bins\n",
    "        hist, bins = np.histogram(primary_props, bins=10)\n",
    "        \n",
    "        print(f\"  Distribution by bins:\")\n",
    "        for i, (start, end) in enumerate(zip(bins[:-1], bins[1:])):\n",
    "            print(f\"    {start:.2f} - {end:.2f}: {hist[i]} samples\")\n",
    "        \n",
    "        # Create a plot of the distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(primary_props, bins=20)\n",
    "        plt.xlabel('Property Value')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Distribution of Primary Property for {class_name}')\n",
    "        plt.savefig(f\"{class_name}_property_distribution.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Suggest values spanning the range\n",
    "        suggested_values = np.linspace(min_val, max_val, 6).tolist()\n",
    "        suggested_values = [round(val) for val in suggested_values]\n",
    "        \n",
    "        print(f\"\\nSuggested property values for generation (spanning the dataset range):\")\n",
    "        print(f\"  {suggested_values}\")\n",
    "        \n",
    "        return min_val, max_val, suggested_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb111a6f-f9a6-40b6-9254-ee399789235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator function\n",
    "def generate_and_save_images(model, scheduler, dataset, epoch, num_samples=4, steps=500, guidance_scale=3.0):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Select random samples from dataset for conditioning\n",
    "        indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "        generated_images = []\n",
    "        property_values = []\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            print(f\"Generating sample {i+1}/{num_samples}\")\n",
    "            \n",
    "            batch = dataset[idx]\n",
    "            properties = batch[\"properties\"].to(device)\n",
    "            norm_prop = batch[\"normalized_prop\"].to(device)\n",
    "            property_values.append(properties.item())\n",
    "            \n",
    "            img_size = 256\n",
    "            \n",
    "            # Create property channel for conditioning\n",
    "            prop_channel = norm_prop.view(1, 1, 1, 1).repeat(1, 1, img_size, img_size).to(device)\n",
    "            \n",
    "            # Start with random noise for the image channel\n",
    "            noise = torch.randn(1, 1, img_size, img_size).to(device)\n",
    "            \n",
    "            # Initialize input with noise for image and property for conditioning\n",
    "            noisy_image = torch.cat([noise, prop_channel], dim=1)\n",
    "            \n",
    "            # Create null property channel for classifier-free guidance\n",
    "            null_channel = torch.zeros_like(prop_channel)\n",
    "            \n",
    "            # Single print for each sample, updated in place\n",
    "            print(f\"Denoising sample {i+1}/{num_samples}: 0%\", end=\"\\r\")\n",
    "            \n",
    "            steps_to_use = min(steps, len(scheduler.timesteps))\n",
    "            for t_idx, t in enumerate(scheduler.timesteps[:steps_to_use]):\n",
    "                # Update progress percentage\n",
    "                progress = (t_idx + 1) / steps_to_use * 100\n",
    "                print(f\"Denoising sample {i+1}/{num_samples}: {progress:.1f}%\", end=\"\\r\")\n",
    "                \n",
    "                # Get model prediction\n",
    "                noise_pred = model(noisy_image, t).sample\n",
    "                \n",
    "                # For classifier-free guidance (if enabled)\n",
    "                if guidance_scale > 1.0:\n",
    "                    # Get unconditional prediction\n",
    "                    unconditional_input = torch.cat([noisy_image[:, :1], null_channel], dim=1)\n",
    "                    unconditional_pred = model(unconditional_input, t).sample\n",
    "                    \n",
    "                    # Apply guidance\n",
    "                    guided_pred = unconditional_pred + guidance_scale * (noise_pred - unconditional_pred)\n",
    "                    \n",
    "                    # Update only the image part, keeping property channel fixed\n",
    "                    updated = scheduler.step(guided_pred[:, :1], t, noisy_image[:, :1]).prev_sample\n",
    "                else:\n",
    "                    # Standard denoising without guidance\n",
    "                    updated = scheduler.step(noise_pred[:, :1], t, noisy_image[:, :1]).prev_sample\n",
    "                \n",
    "                noisy_image = torch.cat([updated, prop_channel], dim=1)\n",
    "                \n",
    "                # Clear GPU cache periodically\n",
    "                if t_idx % 100 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            # Print newline after completion\n",
    "            print()\n",
    "            \n",
    "            # Get only the image channel and clamp values\n",
    "            generated_img = torch.clamp(noisy_image[:, :1], -1, 1)\n",
    "            generated_images.append(generated_img.cpu())\n",
    "            \n",
    "            # Clear GPU cache\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Create a grid of ONLY generated images\n",
    "        fig, axes = plt.subplots(1, num_samples, figsize=(4*num_samples, 4))\n",
    "        \n",
    "        # Handle case of single sample\n",
    "        if num_samples == 1:\n",
    "            axes = np.array([axes])\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Generated image\n",
    "            gen_img = dataset.denormalize_image(generated_images[i].squeeze()).numpy()\n",
    "            \n",
    "            if num_samples > 1:\n",
    "                axes[i].imshow(gen_img, cmap='gray')\n",
    "                axes[i].set_title(f\"Prop: {property_values[i]:.3f}\")\n",
    "                axes[i].axis('off')\n",
    "            else:\n",
    "                axes.imshow(gen_img, cmap='gray')\n",
    "                axes.set_title(f\"Prop: {property_values[i]:.3f}\")\n",
    "                axes.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"generated_samples/epoch_{epoch}.png\", dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Generated samples saved to generated_samples/epoch_{epoch}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3723a-3ca2-42f2-9c62-d1fe1a8e14f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the property conditional diffusion model\n",
    "def train_property_conditional_diffusion():\n",
    "    \"\"\"Train the property conditional diffusion model\"\"\"\n",
    "    # Configuration\n",
    "    file_path = \"../MICRO2D_homogenized.h5\"  # Update to your file path\n",
    "    microstructure_class = 'NBSA'\n",
    "    property_type = 'mechanical'\n",
    "    batch_size = 4  # Reduced batch size\n",
    "    num_epochs = 200  # Increased epochs\n",
    "    learning_rate = 5e-5  # Reduced learning rate\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = MICRO2D_Dataset(file_path, microstructure_class, property_type, augment=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Create model with increased capacity\n",
    "    model = UNet2DModel(\n",
    "        sample_size=256,\n",
    "        in_channels=2,    \n",
    "        out_channels=2,   \n",
    "        layers_per_block=3,  # Increased from 2\n",
    "        block_out_channels=(128, 256, 512, 512),  # Increased capacity\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\", \n",
    "            \"AttnDownBlock2D\",  # Added attention to more layers\n",
    "            \"AttnDownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"AttnUpBlock2D\", \n",
    "            \"AttnUpBlock2D\",  # Added attention to more layers\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\", \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Use more advanced scheduler\n",
    "    noise_scheduler = DDIMScheduler(\n",
    "        num_train_timesteps=1000,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        clip_sample=False,\n",
    "        prediction_type=\"epsilon\"\n",
    "    )\n",
    "    \n",
    "    # Create optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # Create learning rate scheduler\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=500,\n",
    "        num_training_steps=len(dataloader) * num_epochs\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # track epoch progress\n",
    "        epoch_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Simple epoch header\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Training:\")\n",
    "        \n",
    "        for step, batch in enumerate(dataloader):\n",
    "            clean_images = batch[\"pixel_values\"].to(device)\n",
    "            combined_input = batch[\"combined_input\"].to(device)\n",
    "            \n",
    "            # Sample noise\n",
    "            noise = torch.randn(clean_images.shape).to(device)\n",
    "            batch_size = clean_images.shape[0]\n",
    "            \n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, \n",
    "                (batch_size,), device=device\n",
    "            ).long()\n",
    "            \n",
    "            # Add noise to the clean images\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            \n",
    "            # Create noisy combined input (noise applied only to image channel)\n",
    "            noisy_combined = torch.cat([noisy_images, combined_input[:, 1:]], dim=1)\n",
    "            \n",
    "            # Get model prediction\n",
    "            noise_pred = model(noisy_combined, timesteps).sample\n",
    "            \n",
    "            # Calculate loss only on the image channel\n",
    "            loss = F.mse_loss(noise_pred[:, :1], noise)\n",
    "            \n",
    "            # Update model\n",
    "            loss.backward()\n",
    "            \n",
    "            # Add gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            # Print occasional updates without a progress bar\n",
    "            if step % 10 == 0:\n",
    "                print(f\"  Batch {step}/{len(dataloader)} - Loss: {loss.item():.6f}\", end=\"\\r\")\n",
    "            \n",
    "            # Periodically clear cache\n",
    "            if step % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Log average loss for the epoch\n",
    "        avg_epoch_loss = epoch_loss / batch_count\n",
    "        print(f\"\\nEpoch {epoch+1} completed - Average Loss: {avg_epoch_loss:.6f}\")\n",
    "        \n",
    "        # Generate and save images every 10 epochs or on the final epoch\n",
    "        if (epoch + 1) % 10 == 0 or (epoch + 1) == num_epochs:\n",
    "            print(f\"Generating images for epoch {epoch+1}...\")\n",
    "            generate_and_save_images(\n",
    "                model, \n",
    "                noise_scheduler, \n",
    "                dataset, \n",
    "                epoch+1, \n",
    "                steps=250,  # Reduced steps for faster generation\n",
    "                guidance_scale=3.0  # Enable classifier-free guidance\n",
    "            )\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'property_type': property_type,\n",
    "        'property_min': dataset.prop_min,\n",
    "        'property_max': dataset.prop_max,\n",
    "    }, \"models/nbsa_diffusion_final.pt\")\n",
    "    \n",
    "    # Close the dataset HDF5 file\n",
    "    dataset.close()\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a465909d-e787-40e9-bf62-c80a1e81f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate custom microstructures\n",
    "def generate_custom_microstructures(model_path, file_path, property_values, num_samples=4, class_name='NBSA'):\n",
    "    \"\"\"\n",
    "    Generate custom microstructures with specified property values\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model\n",
    "        file_path: Path to the HDF5 file (for dataset info)\n",
    "        property_values: List of property values to condition generation on\n",
    "        num_samples: Number of samples to generate per property value\n",
    "        class_name: Microstructure class to use\n",
    "    \"\"\"\n",
    "    # Load model checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Create model with the improved architecture\n",
    "    model = UNet2DModel(\n",
    "        sample_size=256,\n",
    "        in_channels=2,\n",
    "        out_channels=2,\n",
    "        layers_per_block=3,\n",
    "        block_out_channels=(128, 256, 512, 512),\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\", \n",
    "            \"AttnDownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"AttnUpBlock2D\", \n",
    "            \"AttnUpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\", \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create scheduler - use DDIM for better quality\n",
    "    scheduler = DDIMScheduler(\n",
    "        num_train_timesteps=1000,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        clip_sample=False,\n",
    "        prediction_type=\"epsilon\"\n",
    "    )\n",
    "    \n",
    "    # Create a temporary dataset to get the property normalization\n",
    "    temp_dataset = MICRO2D_Dataset(file_path, class_name)\n",
    "    \n",
    "    # If property_min and property_max are in the checkpoint, use those instead\n",
    "    prop_min = checkpoint.get('property_min', temp_dataset.prop_min)\n",
    "    prop_max = checkpoint.get('property_max', temp_dataset.prop_max)\n",
    "    \n",
    "    # Guidance scale for classifier-free guidance\n",
    "    guidance_scale = 3.0\n",
    "    \n",
    "    # Generate images for each property value\n",
    "    all_generated = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for prop_val in property_values:\n",
    "            print(f\"Generating samples for property value: {prop_val}\")\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                # Create property tensor and normalize it\n",
    "                norm_prop = 2 * (prop_val - prop_min) / (prop_max - prop_min) - 1\n",
    "                prop_tensor = torch.tensor([[norm_prop]], dtype=torch.float32).to(device)\n",
    "                prop_channel = prop_tensor.view(1, 1, 1, 1).repeat(1, 1, 256, 256)\n",
    "                \n",
    "                # Create null property channel for classifier-free guidance\n",
    "                null_channel = torch.zeros_like(prop_channel)\n",
    "                \n",
    "                # Start with random noise\n",
    "                noise = torch.randn(1, 1, 256, 256).to(device)\n",
    "                \n",
    "                # Combine noise with property channel\n",
    "                noisy_image = torch.cat([noise, prop_channel], dim=1)\n",
    "                \n",
    "                # Single print for this sample, updated in place\n",
    "                print(f\"  Sample {i+1}/{num_samples}: 0%\", end=\"\\r\")\n",
    "                \n",
    "                # Sampling loop - more steps for higher quality\n",
    "                steps_to_use = 500\n",
    "                for t_idx, t in enumerate(scheduler.timesteps[:steps_to_use]):\n",
    "                    # Update progress inline\n",
    "                    progress = (t_idx + 1) / steps_to_use * 100\n",
    "                    print(f\"  Sample {i+1}/{num_samples}: {progress:.1f}%\", end=\"\\r\")\n",
    "                    \n",
    "                    # Get conditional prediction\n",
    "                    noise_pred = model(noisy_image, t).sample\n",
    "                    \n",
    "                    # For classifier-free guidance\n",
    "                    if guidance_scale > 1.0:\n",
    "                        # Get unconditional prediction\n",
    "                        unconditional_input = torch.cat([noisy_image[:, :1], null_channel], dim=1)\n",
    "                        unconditional_pred = model(unconditional_input, t).sample\n",
    "                        \n",
    "                        # Apply guidance\n",
    "                        guided_pred = unconditional_pred + guidance_scale * (noise_pred - unconditional_pred)\n",
    "                        \n",
    "                        # Update only the image part, keeping property channel fixed\n",
    "                        updated = scheduler.step(guided_pred[:, :1], t, noisy_image[:, :1]).prev_sample\n",
    "                    else:\n",
    "                        # Standard denoising without guidance\n",
    "                        updated = scheduler.step(noise_pred[:, :1], t, noisy_image[:, :1]).prev_sample\n",
    "                    \n",
    "                    noisy_image = torch.cat([updated, prop_channel], dim=1)\n",
    "                \n",
    "                print()  # New line after completion\n",
    "                \n",
    "                # Get only the image channel and clamp values\n",
    "                generated_img = torch.clamp(noisy_image[:, :1], -1, 1).cpu()\n",
    "                all_generated.append((prop_val, generated_img))\n",
    "                \n",
    "                # Clear GPU cache\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Create a grid of images\n",
    "        rows = len(property_values)\n",
    "        cols = num_samples\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(3*cols, 3*rows))\n",
    "        \n",
    "        # Handle single row/column cases\n",
    "        if rows == 1 and cols == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        elif cols == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        for row, prop_val in enumerate(property_values):\n",
    "            for col in range(num_samples):\n",
    "                idx = row * num_samples + col\n",
    "                prop, img = all_generated[idx]\n",
    "                \n",
    "                # Display image\n",
    "                axes[row, col].imshow(temp_dataset.denormalize_image(img.squeeze()).numpy(), cmap='gray')\n",
    "                axes[row, col].set_title(f\"Prop: {prop:.2f}\")\n",
    "                axes[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"generated_samples/custom_properties.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # Close the dataset\n",
    "    temp_dataset.close()\n",
    "    \n",
    "    print(\"Custom microstructure generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c9277-936a-4796-9a6b-e6b8f5184662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property statistics for NBSA (mechanical):\n",
      "  Min: 1706.90\n",
      "  Max: 2364.29\n",
      "  Mean: 1944.66\n",
      "  Median: 1936.45\n",
      "  Distribution by bins:\n",
      "    1706.90 - 1772.64: 88 samples\n",
      "    1772.64 - 1838.37: 289 samples\n",
      "    1838.37 - 1904.11: 294 samples\n",
      "    1904.11 - 1969.85: 288 samples\n",
      "    1969.85 - 2035.59: 299 samples\n",
      "    2035.59 - 2101.33: 204 samples\n",
      "    2101.33 - 2167.07: 113 samples\n",
      "    2167.07 - 2232.81: 44 samples\n",
      "    2232.81 - 2298.55: 11 samples\n",
      "    2298.55 - 2364.29: 4 samples\n",
      "\n",
      "Suggested property values for generation (spanning the dataset range):\n",
      "  [1707, 1838, 1970, 2101, 2233, 2364]\n",
      "Loaded 1634 NBSA microstructures\n",
      "Property shape: (1634, 6, 5)\n",
      "Property range: 1706.90 to 2364.29\n",
      "Epoch 1/200 - Training:\n",
      "  Batch 400/409 - Loss: 0.083586\n",
      "Epoch 1 completed - Average Loss: 0.282838\n",
      "Epoch 2/200 - Training:\n",
      "  Batch 400/409 - Loss: 0.050484\n",
      "Epoch 2 completed - Average Loss: 0.093696\n",
      "Epoch 3/200 - Training:\n",
      "  Batch 400/409 - Loss: 0.047192\n",
      "Epoch 3 completed - Average Loss: 0.066010\n",
      "Epoch 4/200 - Training:\n",
      "  Batch 400/409 - Loss: 0.037959\n",
      "Epoch 4 completed - Average Loss: 0.058884\n",
      "Epoch 5/200 - Training:\n",
      "  Batch 400/409 - Loss: 0.043701\n",
      "Epoch 5 completed - Average Loss: 0.055197\n",
      "Epoch 6/200 - Training:\n",
      "  Batch 400/409 - Loss: 0.093261\n",
      "Epoch 6 completed - Average Loss: 0.051298\n",
      "Epoch 7/200 - Training:\n",
      "  Batch 400/409 - Loss: 0.039549\n",
      "Epoch 7 completed - Average Loss: 0.049407\n",
      "Epoch 8/200 - Training:\n",
      "  Batch 400/409 - Loss: 0.065395\n",
      "Epoch 8 completed - Average Loss: 0.050079\n",
      "Epoch 9/200 - Training:\n",
      "  Batch 400/409 - Loss: 0.028611\n",
      "Epoch 9 completed - Average Loss: 0.048770\n",
      "Epoch 10/200 - Training:\n",
      "  Batch 140/409 - Loss: 0.056378\r"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Analyze property distribution first\n",
    "    min_val, max_val, suggested_values = analyze_property_distribution(\n",
    "        file_path=\"../MICRO2D_homogenized.h5\",\n",
    "        class_name='NBSA',\n",
    "        property_type='mechanical'\n",
    "    )\n",
    "    \n",
    "    # Use the suggested values for generation\n",
    "    property_values = suggested_values\n",
    "    # Train the model\n",
    "    train_property_conditional_diffusion()\n",
    "\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    # Generate custom microstructures\n",
    "    generate_custom_microstructures(\n",
    "         model_path=\"models/nbsa_diffusion_final.pt\",\n",
    "        file_path=\"../MICRO2D_homogenized.h5\",\n",
    "            property_values=property_values,\n",
    "            num_samples=4\n",
    "        )\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972cdd01-abc5-4b54-87b1-1957fe3f77e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (diffusion)",
   "language": "python",
   "name": "diffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
